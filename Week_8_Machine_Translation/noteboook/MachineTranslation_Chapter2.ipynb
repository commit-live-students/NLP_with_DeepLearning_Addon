{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq models use neural networks to translate a piece of text from a source language to a machine language. Introduced by Google, it has proved itself for a variety of applications, namely, machine translation, image captioning, conversation models, text summarization etc. \n",
    "\n",
    "As the name sugests, it injests words in sequence and translates them. The beauty of seq2seq is that it not only considers the word, but it also considers the neighbouring words to capture better semantics. It uses RNN to learn and then translate an unseen piece of text from one language to another. More often than not, some specialized RNNs like LSTM or GRU is used since the vanilla RNN suffers from the problem of vanishing gradients. \n",
    "\n",
    "Seq2Seq models use a special arrangements of neural networks, viz, __Encoder Decoder__ networks to translate. The Encoder takes in the source text and outputs an intermediate vector which represents the input sequence. The decoder then reads the intermediate vector output of the encoder and then translates it into the target language. The following figure gives a pictorial representation of seq2seq model.\n",
    "\n",
    "<img src=\"../images/encodec_overview.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "1. A stack of several recurrent units (LSTM or GRU cells for better performance) where each accepts a single element of the input sequence, collects information for that element and propagates it forward.\n",
    "\n",
    "2. The input sequence is a collection of all words from the question. Each word is represented as x_i where i is the order of that word. The words are then encoded into dense vectors to represt the input text. This vector aims to encapsulate the information for all input elements in order to help the decoder make accurate predictions.It acts as the initial hidden state of the decoder part of the model.\n",
    "\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. A stack of several recurrent units where each predicts an output y_t at a time step t.\n",
    "\n",
    "2. Each recurrent unit accepts a hidden state from the previous unit(Encoder unit) and produces and output as well as its own hidden state.\n",
    "\n",
    "3. The output sequence is a collection of all words from the answer. Each word is represented as y_i where i is the order of that word.\n",
    "\n",
    "The power of this model lies in the fact that it can map sequences of different lengths to each other. As you can see the inputs and outputs are not correlated and their lengths can differ. This opens a whole new range of problems which can now be solved using such architecture.\n",
    "\n",
    "\n",
    "The Encoder - Decoder model not only gets better accuracy, but is also much scalable than any of the traditional machine translation methods know thus far. Infact, seq2seq is so popular and effective that any modern day machine translation happens via this. Google translate is one of the most popular platforms relying on seq2seq for translating text in different languages.\n",
    "\n",
    "### Uses in Industry\n",
    "\n",
    "Seq2seq has proved its mettle and is being used in lot of areas to solve complex problems. Let's take a look at some of these.\n",
    "\n",
    "#### Machine Translation : - \n",
    "\n",
    "Used to translate text from one language to another language. We will study this is more detail in later sections.\n",
    "\n",
    "#### Image Captioning :-\n",
    "\n",
    "Image Captioning is a technique, where in computer automatically generates captions for an image. For example if there is a picture in which a cat is sitting on a table, computer would generate a caption like \"Cat sitting on table\". \n",
    "\n",
    "For those of use with knowledge / interest in convulational neural networks, the image is converted into vectors using conv-nets, and then the resulting vector is trained on seq2seq models. The same can be envisioned using the image below:-\n",
    "\n",
    "<img src=\"../images/img_captioning.jpg\"/>\n",
    "\n",
    "#### Other uses\n",
    "\n",
    "Seq2seq finds additional uses in :-\n",
    "\n",
    "    *Conversation models (for chatbots etc)\n",
    "    \n",
    "    *Document sumarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq in action\n",
    "\n",
    "Having seen how seq2seq works at a very high level, let us try and understand how do we actually translate a text from a source language to a target language. \n",
    "\n",
    "For our study let us consider the following sentence in Spanish :\n",
    "\n",
    "__Patrick visitará la India en septiembre.__\n",
    "\n",
    "The task at hand is to translate this sentence into English. However we do have some problems to solve first.\n",
    "\n",
    "1. To translate the text correctly into the target language.\n",
    "\n",
    "2. To check if the translated text is correct.\n",
    "\n",
    "3. To select the best available translation(after 1 & 2) of the target language.\n",
    "\n",
    "Point 1 is pretty clear in itself. However, 2 brings with itself additional problems,i.e, how do we verify if the translated text is correct. Point 3 futher states that, assuming if the translation(s) are correct, which one do we select. How do we determine the accuracy of the translation. Forexample, let us look at the following translations of the above text:\n",
    "\n",
    "1. Patrick will visit India in September.\n",
    "\n",
    "2. Patrick is going to visit India in September.\n",
    "\n",
    "3. In September, Patrick will be  visiting India.\n",
    "\n",
    "4. In September, Patrick is welcome in India.\n",
    "\n",
    "Translations from 1-3 are more or less correct and they kind of convey the meaning of the source language. However, translation number 1 is the best out of all the 4 because it conveys the meaning of the source text precisely and concisely and also is grammatically correct. However, translation 4 goes awfully wrong. We will see how tackle / solve the above problems one by one.\n",
    "\n",
    "Let us try and define the objective now. The objective of a machine translation model is to translate a given piece of text in one language to its best possible match in the target language. Or in other words, find a piece of text in the target language whose probablity of being a correct translation of the piece of text in the source language is the highest. Mathematically, we can say that we have to find translation(text in target language) which has the highest conditional probablity of being the correct translation given the source text. In another words we need to find the value of $Y_i$ which maximizes $P(Y|X)$ ,where $Y$ is a set of all the probable candidates for the correct translation and $X$ is the source text. To find the words from the target dictionary, we use a specialized algorithm called Beam Search. \n",
    "\n",
    "__Beam Search__\n",
    "\n",
    "In order to translate successfully, we need to find the words to begin with. Beam search is a specially designed algorithm to help us with that. It will find the most probable word(s) from the dictionary of the target language that will be used in the translated text. Let's take a look at the image below:-\n",
    "\n",
    "<img src=\"../images/beamsearch_step1.png\"/>\n",
    "\n",
    "\n",
    "Following are the actions performed by the beam search algorithm to find out the words from the target language.\n",
    "\n",
    "__Step 1__:\n",
    "\n",
    "1. Given dictionary of the target language & the beam width (explained in the figure), find out the probablity of the all the words from the dictionary given the source text. \n",
    "\n",
    "    $P(Y_i|X)$ , where $Y_i$ are the words from the dictionary and $X$ is the source text to be translated.\n",
    "    \n",
    "    So essentially the beam search algorithm would take all the words in the dictionary of the target language and run it through a softmax layer over an encoder-decoder network, essentially calculating conditinal probability of all the words in the dictionary so as to choose the best first word.\n",
    "    \n",
    "    <img src=\"../images/beamsearch2.png\"/>\n",
    "\n",
    "2. Select top n = beam width words which have the highest conditional probablity. In this case let us say the top three words that the beam search algorithm picks up is \"patrick\", \"in\", \"september\".\n",
    "\n",
    "\n",
    "__Step 2__:\n",
    "\n",
    "1. After getting the most probable first words for the translation, the beam search algorithms needs to find the next words. To do that,\n",
    "    * It keeps the first words in memory(number of words equal to the max beam length) calculates the probablity of the second word given the first word and the input sentence. An intuition can be developed from the image below:-\n",
    "    <img src=\"../images/beamsearch_s20.png\"/>\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/beamsearch_step2.png\"/>\n",
    "\n",
    "\n",
    "As we can see from the image above, the algorithm now tries to find out the second word keeping the first set of words fixed (from step 1). Mathematically, in this step we are trying to maximize the probablity of two words given the input sentence.\n",
    "$P(Y_1,Y_2|X)$\n",
    "\n",
    "By the rules of conditional probablity, this can also be written as:-\n",
    "\n",
    "$P(Y_1,Y_2|X)=P(Y_1|X)*P(Y_2|X)$\n",
    "\n",
    "When the above operation is carried out on all the words of the dictionary one by one, we get the pair of words with the highest probablity as probable candidates for the translation.\n",
    "\n",
    "The above equation can also be extented to n terms, for example\n",
    "\n",
    "$P(Y_1...Y_n|X)=P(Y_1|X)*P(Y_2|X)...P(Y_n|X)$\n",
    "\n",
    "The above steps are repeated until all the probable words for the translation are found out by the algorithm. So the entire process when combined might look something like this:-\n",
    "\n",
    "<img src=\"../images/machine_translation.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation Evaluation: Bleu Score (Bilingual evaluation understudy)\n",
    "\n",
    "Now, once we have a proper translation, how do we verify the results? To evaluate a MT model, the standard way is to calculate the Blue score. However, The standard way to calculate the accuracy of a ML model is by calculating __Precision__. Let's see why calculating Precision will not work in our example. So let's take our translation example:-\n",
    "\n",
    "Spanish : Patrick visitará la India en septiembre.\n",
    "\n",
    "Some human verified references :\n",
    "\n",
    "1. Patrick will visit India in September.\n",
    "\n",
    "2. Patrick is going to visit India in September\n",
    "\n",
    "\n",
    "Now, let's take a look at what a machine translated output could look like in extreme cases :-\n",
    "\n",
    "1. Patrick will visit India in September.\n",
    "\n",
    "2. visit visit visit visit visit visit\n",
    "\n",
    "Now if we were to calculate the Precision of the above outputs, it would give some unexpected results. Like, let's calculate the precision of machine translated output #2 \n",
    "\n",
    "Recall that precision is number of correct predections (In this case a prediction is correct is the word is present in the human reference ) divided by total correct predictions. So in our reference num 1 , we see there are 6 words, and from the machine translated output, we see that all the 6 words (visit) is present in the human reference. So this essentially gives us a precision of 1 for a poorly translated output.\n",
    "\n",
    "It is for this problem, a new measure for calculating the effectiveness of machine translation was invented.\n",
    "\n",
    "Blue score is calculated in the following way:-\n",
    "\n",
    "1. n-Grams are considered for calculating the score.\n",
    "\n",
    "2. Number of occurance of n-Grams(in the above example, unigrams ) is capped to the maximum number of occurance of the n-Grams in any of the references.\n",
    "\n",
    "\n",
    "Having gained insight into how seq2seq model works, let's try and implement it in python in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation in Python\n",
    "\n",
    "In the last chapter we learnt about Seq2Seq modelling for machine translation. In this chapter we will actually learn how to implement a machine learning model in Python. We will learn how to use translate sentence from a source language to another language by building a Seq2Seq model using Keras, a popular python deep learning API. \n",
    "\n",
    "Here is what we are going to do:\n",
    "\n",
    "1. Turn each of the sentence into 3 Numpy arrays, __encoder_input_data, decoder_input_data, decoder_target_data__:\n",
    "\n",
    "__encoder_input_data__ is a 3D array of shape (num_pairs, max_jap_sentence_length, num_jap_characters) containing a one-hot vectorization of the Japanese sentences.\n",
    "\n",
    "__decoder_input_data__ is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
    "\n",
    "__decoder_target_data__ is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n",
    "\n",
    "2. Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data. Our model uses teacher forcing.\n",
    "\n",
    "3. Decode some sentences to check that the model is working (i.e. turn samples from encoder_input_data into corresponding samples from decoder_target_data).\n",
    "\n",
    "\n",
    "Because the training process and inference process (decoding sentences) are quite different, we use different models for both, i.e, they all leverage the same inner layers.\n",
    "\n",
    "This is our training model. It leverages three key features of Keras RNNs:\n",
    "\n",
    "1. The return_state contructor argument, configuring a RNN layer to return a list where the first entry is the outputs and the next entries are the internal RNN states. This is used to recover the states of the encoder.\n",
    "\n",
    "2. The inital_state call argument, specifying the initial state(s) of a RNN. This is used to pass the encoder states to the decoder as initial states.\n",
    "\n",
    "3. The return_sequences constructor argument, configuring a RNN to return its full sequence of outputs (instead of just the last output, which the defaults behavior). This is used in the decoder.\n",
    "\n",
    "Knowing the overview of we are trying to achieve, let's understand the code snippets that we can use the build the Keras  model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and formost, we import the libraries that we will be needing. Following snippet does that:-\n",
    "\n",
    "```python\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "```\n",
    "\n",
    "In the snippet below, we define the Encoder Inputs as a Keras Input Layer. The keras input layer is passed on parameters about the shape of the input and the number of word tokens in the input sequence.\n",
    "\n",
    "We also define an LSTM model `encoder`.\n",
    "\n",
    "We then call the encoder function to get the encoder output and the hidden state variables. We discard encoder_output and keep the state variables, namely `state_h` and `state_c` for further processing.\n",
    "```python\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "```\n",
    "\n",
    "We will not setup the decoder network to decode from the encoder states. Following are the things that we do in this phase:\n",
    "\n",
    "1. Set up the decoder, using `encoder_states` as initial state.\n",
    "\n",
    "2. We set up our decoder to return full output sequences,and to return internal states as well. We don't use the \n",
    "return states in the training model, but we will use them in inference.\n",
    "\n",
    "Following snippet shows the process in more detail.\n",
    "\n",
    "``` python\n",
    "\n",
    "# Setup Decoder network\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "```\n",
    "\n",
    "Once we have the decoder layer setup, we now define a Keras model that takes in the `encoder_input_data` and `decoder_inputs` and outputs the `decoder_target_data`.\n",
    "\n",
    "```python\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "```\n",
    "\n",
    "Once we have the model, we run training on the model to train it on the input Japanese data. Following snippet does that:-\n",
    "\n",
    "\n",
    "``` python\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "```\n",
    "\n",
    "Once we have the trained model, we will use this model to decode the sentence and print out the output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope the above section of implementing a machine translation using python brings some clarity on how can you implement your own machine translation system. In the section below, we will now actually implement the above concept on a sample dataset. The dataset contains English - French translation. On the lines described above, let's start to build an encoder decoder model using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "# Setting training environments and other variables\n",
    "train_batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim=256\n",
    "num_samples=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Go.</th>\n",
       "      <th>Va !</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>Au feu !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Help!</td>\n",
       "      <td>À l'aide !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Go.        Va !\n",
       "0   Run!     Cours !\n",
       "1   Run!    Courez !\n",
       "2   Wow!  Ça alors !\n",
       "3  Fire!    Au feu !\n",
       "4  Help!  À l'aide !"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the dataset a bit, aye ?\n",
    "df = pd.read_csv(\"../data/frenchenglish-bilingual-pairs/fra-eng/fra.txt\",delimiter='\\t')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataset is a tad delimited file of Eglish - French translation. Let us now prepare this dataset to suit the parameters required to use Keras apis as described in the section above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preparing data for training\n"
     ]
    }
   ],
   "source": [
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_chars=set()\n",
    "target_chars=set()\n",
    "lines = open(\"../data/frenchenglish-bilingual-pairs/fra-eng/fra.txt\",encoding='utf-8').read().split(\"\\n\")\n",
    "for line in lines:\n",
    "    try:\n",
    "        input_text, target_text= line.split(\"\\t\")\n",
    "        target_text = '\\t' + target_text + '\\n'\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "\n",
    "        for char in input_text:\n",
    "            if char not in input_chars:\n",
    "                input_chars.add(char)\n",
    "\n",
    "        # Create a set of all unique output characters\n",
    "        for char in target_text:\n",
    "            if char not in target_chars:\n",
    "                target_chars.add(char)\n",
    "\n",
    "       \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "input_chars = sorted(list(input_chars))\n",
    "target_chars = sorted(list(target_chars))\n",
    "num_encoder_tokens = len(input_chars) # aka size of the english alphabet + numbers, signs, etc.\n",
    "num_decoder_tokens = len(target_chars) # aka size of the french alphabet + numbers, signs, etc.\n",
    "\n",
    "                      \n",
    "input_token_index = {char: i for i, char in enumerate(input_chars)}\n",
    "target_token_index = {char: i for i, char in enumerate(target_chars)}\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) # Get longest sequences length\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "\n",
    "# encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) \n",
    "# containing a one-hot vectorization of the English sentences.\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) \n",
    "# containg a one-hot vectorization of the French sentences.\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_target_data is the same as decoder_input_data but offset by one timestep. \n",
    "# decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :]\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "# Loop over input texts\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # Loop over each char in an input text\n",
    "    for t, char in enumerate(input_text):\n",
    "        # Create one hot encoding by setting the index to 1\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    # Loop over each char in the output text\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "print(\"Done preparing data for training\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once we have the dataset prepared as per the requirement of Keras APIs, we will now train the model as per the explanation above:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name = 'encoder_inputs')\n",
    "\n",
    "# The return_state contructor argument, configuring a RNN layer to return a list \n",
    "# where the first entry is the outputs and the next entries are the internal RNN states. \n",
    "# This is used to recover the states of the encoder.\n",
    "encoder = LSTM(latent_dim, \n",
    "                    return_state=True, \n",
    "                    name = 'encoder')\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), \n",
    "                       name = 'decoder_inputs')\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, \n",
    "                         return_sequences=True, \n",
    "                         return_state=True, \n",
    "                         name = 'decoder_lstm')\n",
    "\n",
    "# The inital_state call argument, specifying the initial state(s) of a RNN. \n",
    "# This is used to pass the encoder states to the decoder as initial states.\n",
    "# Basically making the first memory of the decoder the encoded semantics\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, \n",
    "                      activation='softmax', \n",
    "                      name = 'decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 116349 samples, validate on 29088 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "                    decoder_target_data,\n",
    "                    batch_size=1000,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "# Save model\n",
    "#model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the inference mode. In this we do the following:\n",
    "\n",
    "1. Encode input and retrieve initial decoder state\n",
    "\n",
    "2. Run 1 step of decoder with this initial state and a start sequence as target. The output of this will be the next target token\n",
    "\n",
    "3. Repeat the current target token and the current states.\n",
    "\n",
    "Let's see how to implement this in the following section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "#define decoder initial states\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "#define decoder output seq and model\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the inference model ready,  we gotta write a function that will convert the decoded output (which is basically location of the words) to human readable translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take our model for a spin and translate some of the English sentences to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: It's me!\n",
      "Decoded sentence: Je sis sre.\n",
      "\n",
      "-\n",
      "Input sentence: Join us.\n",
      "Decoded sentence: Noss eses\n",
      "\n",
      "-\n",
      "Input sentence: Join us.\n",
      "Decoded sentence: Noss eses\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: Aasee--oi\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: Aasee--oi\n",
      "\n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: Aosee--oi\n",
      "\n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: Aosee--oi\n",
      "\n",
      "-\n",
      "Input sentence: Me, too.\n",
      "Decoded sentence: Noss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: Open up.\n",
      "Decoded sentence: Aasee-oi\n",
      "\n",
      "-\n",
      "Input sentence: Open up.\n",
      "Decoded sentence: Aasee-oi\n",
      "\n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: Aaisee !\n",
      "\n",
      "-\n",
      "Input sentence: See you.\n",
      "Decoded sentence: Aossees s\n",
      "\n",
      "-\n",
      "Input sentence: Show me.\n",
      "Decoded sentence: Noss eses\n",
      "\n",
      "-\n",
      "Input sentence: Show me.\n",
      "Decoded sentence: Noss eses\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: So long.\n",
      "Decoded sentence: Aosseses \n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Tell me.\n",
      "Decoded sentence: Aosee--oi\n",
      "\n",
      "-\n",
      "Input sentence: Tell me.\n",
      "Decoded sentence: Aosee--oi\n",
      "\n",
      "-\n",
      "Input sentence: Tom won.\n",
      "Decoded sentence: Joss sesn.\n",
      "\n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: Aossee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: Aossee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: Aossee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Wake up.\n",
      "Decoded sentence: Aossee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Wake up.\n",
      "Decoded sentence: Aossee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: Aosseses \n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: Aosseses \n",
      "\n",
      "-\n",
      "Input sentence: We know.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: We lost.\n",
      "Decoded sentence: Joss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: Who won?\n",
      "Decoded sentence: Noss sesn.\n",
      "\n",
      "-\n",
      "Input sentence: Who won?\n",
      "Decoded sentence: Noss sesn.\n",
      "\n",
      "-\n",
      "Input sentence: You run.\n",
      "Decoded sentence: Asees e !\n",
      "\n",
      "-\n",
      "Input sentence: Am I fat?\n",
      "Decoded sentence: Ausse-oos\n",
      "\n",
      "-\n",
      "Input sentence: Am I fat?\n",
      "Decoded sentence: Ausse-oos\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Be a man.\n",
      "Decoded sentence: Noss sesn.\n",
      "\n",
      "-\n",
      "Input sentence: Be a man.\n",
      "Decoded sentence: Noss sesn.\n",
      "\n",
      "-\n",
      "Input sentence: Be still.\n",
      "Decoded sentence: Noss eses\n",
      "\n",
      "-\n",
      "Input sentence: Be still.\n",
      "Decoded sentence: Noss eses\n",
      "\n",
      "-\n",
      "Input sentence: Be still.\n",
      "Decoded sentence: Noss eses\n",
      "\n",
      "-\n",
      "Input sentence: Beats me.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Beats me.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Call Tom.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Call Tom.\n",
      "Decoded sentence: Aasee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Cheer up!\n",
      "Decoded sentence: Aosee--oi\n",
      "\n",
      "-\n",
      "Input sentence: Cool off!\n",
      "Decoded sentence: Aosee--ooi\n",
      "\n",
      "-\n",
      "Input sentence: Cuff him.\n",
      "Decoded sentence: Aosee--ois\n",
      "\n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: Aosse-eo\n",
      "\n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: Aosse-eo\n",
      "\n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: Aosse-eo\n",
      "\n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: Aosse-eo\n",
      "\n",
      "-\n",
      "Input sentence: Get down!\n",
      "Decoded sentence: Aleee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Get down.\n",
      "Decoded sentence: Aleee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Get down.\n",
      "Decoded sentence: Aleee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Get down.\n",
      "Decoded sentence: Aleee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Get down.\n",
      "Decoded sentence: Aleee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: Aaisee !\n",
      "\n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: Aaisee !\n",
      "\n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: Aaisee !\n",
      "\n",
      "-\n",
      "Input sentence: Get real!\n",
      "Decoded sentence: Aaisee !\n",
      "\n",
      "-\n",
      "Input sentence: Go ahead.\n",
      "Decoded sentence: Aleee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Go ahead.\n",
      "Decoded sentence: Aleee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Go ahead.\n",
      "Decoded sentence: Aleee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Go ahead.\n",
      "Decoded sentence: Aleee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Good job!\n",
      "Decoded sentence: Aleee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Good job!\n",
      "Decoded sentence: Aleee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Good job!\n",
      "Decoded sentence: Aleee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Grab him.\n",
      "Decoded sentence: Aleee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Grab him.\n",
      "Decoded sentence: Aleee-ooi\n",
      "\n",
      "-\n",
      "Input sentence: Have fun.\n",
      "Decoded sentence: Aossee-oo\n",
      "\n",
      "-\n",
      "Input sentence: Have fun.\n",
      "Decoded sentence: Aossee-oo\n",
      "\n",
      "-\n",
      "Input sentence: He tries.\n",
      "Decoded sentence: Aosseses \n",
      "\n",
      "-\n",
      "Input sentence: He's wet.\n",
      "Decoded sentence: Jess sais.\n",
      "\n",
      "-\n",
      "Input sentence: Hi, guys.\n",
      "Decoded sentence: Noss sasn.\n",
      "\n",
      "-\n",
      "Input sentence: How cute!\n",
      "Decoded sentence: Noss sess\n",
      "\n",
      "-\n",
      "Input sentence: How deep?\n",
      "Decoded sentence: Noss sess\n",
      "\n",
      "-\n",
      "Input sentence: How nice!\n",
      "Decoded sentence: Noss sesn.\n",
      "\n",
      "-\n",
      "Input sentence: How nice!\n",
      "Decoded sentence: Noss sesn.\n",
      "\n",
      "-\n",
      "Input sentence: How nice!\n",
      "Decoded sentence: Noss sesn.\n",
      "\n",
      "-\n",
      "Input sentence: How nice!\n",
      "Decoded sentence: Noss sesn.\n",
      "\n",
      "-\n",
      "Input sentence: Humor me.\n",
      "Decoded sentence: Noss sess\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: Aasee-le\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: Aasee-le\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
