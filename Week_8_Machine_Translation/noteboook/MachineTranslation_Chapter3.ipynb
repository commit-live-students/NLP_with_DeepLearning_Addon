{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model\n",
    "\n",
    "In the last chapter, we learnt about Machine Translation and how to do it in Python. In this chapter, we will learn another very important concept called Attention Mechanism. Attention mechanisms have gained much popularity in recent times because of using them has increased the accuracy of the Encoder-Decoder RNN model.\n",
    "\n",
    "__What is Attention Model?__\n",
    "\n",
    "Well, attention model was majorly proposed as a solution to the traditional Encoder-Decoder model in machine translation. Recall that the Encoder-Decoder model used to encode a fixed length vector injesting all the input data at once to feed it to the Decoder which would then decode all the input (output of the decoder) at once.\n",
    "\n",
    "__Why Attention Models?__\n",
    "\n",
    "Now compare it to a typical problem of machine translation. Imagine if you or any human were to translate the a given input sentence to a target output language. If you were reading the complete sentence at once and translating it to the target language, you would probably do good for short sentences, like < 10 words, but would you be efficient if the input sentences were long ? No, right. Ironically the same behaviour was observed when traditional machine translation models were used in translation. It was found that the accuracy of the model was okay till about a certain length of the input sentence. However, when the length of the input sentence increased, the accuracy of the MT models started going down. With an increase in the number of words in the input text, the accuracy of the model in correctly translating the input sentence started going down. The problem stems from the fixed-length internal representation that must be used to decode each word in the output sequence. The solution is the use of an attention mechanism that allows the model to learn where to place attention on the input sequence as each word of the output sequence is decoded.\n",
    "\n",
    "This could be explained from the following figure:-\n",
    "\n",
    "<img src=\"../images/attention_1.png\"/>\n",
    "\n",
    "The key benefit to the approach is that a single system can be directly trained directly on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning.\n",
    "\n",
    "> Unlike the traditional phrase-based translation system which consists of many small sub-components that are tuned separately, neural machine translation attempts to build and train a single, large neural network that reads a sentence and outputs a correct translation.\n",
    "\n",
    "— [Neural Machine Translation by Jointly Learning to Align and Translate, 2014](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    ">\n",
    "The strength of NMT lies in its ability to learn directly, in an end-to-end fashion, the mapping from input text to associated output text.\n",
    "\n",
    "— [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, 2016.](https://arxiv.org/abs/1609.08144)\n",
    "\n",
    "__Advantages__\n",
    "\n",
    "Attention models address this exact problem faced in traditional MT models. Let's try to understand this with the human translating sentence example we saw earlier. Now typically when a human gets a long input sentence, when he/she would sub-consciously do is break the sentence into parts, like of 4-5 words, read them and translate it into target language, and then move on to the next chunk of sentence. The way these small chunks are decided completely depends on person to person, but the idea is that a large sentence is then broken down into smaller sentence and then each of the smaller sentence is translated. It was observed that using this approach, the MT accuracy was high, even for sentences with large number of words.\n",
    "\n",
    "<img src=\"../images/attention_2.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work ?\n",
    "\n",
    "In this section let's try to understand how does attention model work. How is it able to maintain accuracy of the prediction over a large input sentence. To do that lets take a look at the following figure, which extends to our last learnt concept of encoder-decoder networks.\n",
    "\n",
    "<img src=\"../images/attention_3.png\"/>\n",
    "\n",
    "The way the Attention Model differs from typical Encoder Decoder model is that instead of the decoder network directly decoding the output of the Encoder network, it is not passed on a Context vector $C$ which is a combination of output of the Encoder network $\\alpha$ and an attention parameter $A$. The Context vector $C$ is then passed on to the decoder network to get the final output. Let's try to understand this process in more detail.\n",
    "\n",
    "1. As you can see in the figure above, attention parameter is calculated for each of the input sequences. We denote this attention parameter using $\\alpha$. $\\alpha_i$ denotes the amount of attention that you need to pay to its corresponding input vector $x_i$. To get an intuition, think of it as a complex function which determines the weight of the input sequence. \n",
    "The way you calculate $\\alpha_i$ is to train a small neural network which basically takes in two parameters:-\n",
    "\n",
    "    1. The output of the encoder network of a particular input sequence.\n",
    "    \n",
    "    2. The hidden state of the encoder network (which we were not considering at all in the traditional Encoder- Decoder) network.\n",
    "    \n",
    "    The following figure demonstrates the above concept. \n",
    "    \n",
    "    <img src=\"../images/attentionparam.png\"/>\n",
    "\n",
    "2. Once you have the attention parameter $\\alpha$, we now compute a Context vector which is nothing but a weighted sum of the product of:\n",
    "    1. Output of the encoder netework.\n",
    "    \n",
    "    2. Attention parameter.\n",
    "    \n",
    "    $C=\\sum_{i=1}^n\\alpha_i*A_i$\n",
    "    \n",
    "3. The Context vector $C$ is then fed into the deocder network to predict the output. To predict the output, we use a feed forward neural network, which takes in the Context Vectors (from step 2 ) as the parameters and then predicts the decoded output of the network. The following figures attempts to give an intuition on the same.\n",
    "\n",
    "<img src = \"../images/decoder_attention.png\"/>\n",
    "\n",
    "Notice here that the input to the decoder is now an attention parameter which indicates how much of attention to give to each of the words in the input sequence and the output of the encoder. This way the decoder network is able to decode the input sequence in batches (since the attention parameter tends to zero for far off words. Which makes sense also since usuallly the far off words do not carry the context or any relation with the current word in a language.)\n",
    "\n",
    "Now let's look at how do we implement attention model using tensorflow in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-French dataset like in the previous chapter. Here is a breif description of the process that we are going to follow to prepare the dataset.\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "\n",
    "2. Clean the sentences by removing special characters.\n",
    "\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "\n",
    "4. Pad each sentence to a maximum length.\n",
    "\n",
    "As we saw in the last tutorial let's load the dataset for processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-40a4d3a5f2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "\n",
    "path_to_file=\"../data/frenchenglish-bilingual-pairs/fra-eng/fra.txt\"\n",
    "df = pd.read_csv(\"../data/frenchenglish-bilingual-pairs/fra-eng/fra.txt\",delimiter='\\t')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now preprocess the dataset to be used to Model Consumption.\n",
    "\n",
    "## The following class will create a dictionary of words for the dataset. \n",
    "## The dictionary will be in the form of ID-> WORD structure. Forexample, \"mom\"->7\n",
    "\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "    \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "            \n",
    "## Load the dataset in proper format\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "    targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # French sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    # English sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n",
    "\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now write our Encoder Decoder model. Also for this chapter we will be using GRU instead of LSTM for simplicity since GRU has just one state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let up define GRU units for calculation\n",
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a loss function to train our Encoder\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this brings us to the conclusion of the chapter on Attention mechanism. Let us try and answer the following questions to test our undersanding on Attention MEchanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As a datascientist when using machine translation algorithms, you find that the accuracy of your model is decreasing and also the length of your input sentences is increasing. What would you do ?\n",
    "\n",
    "1. Use recursive units instead of recurrent\n",
    "\n",
    "2. Use attention mechanism\n",
    "\n",
    "3. Use character level translation\n",
    "\n",
    "4. None of these\n",
    "\n",
    "Solution: __2__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The network learns to pay attention by learning the values of Context vector. Can we train a small NN to get the context vectors?\n",
    "\n",
    "1. True\n",
    "\n",
    "2. False\n",
    "\n",
    "Solution:__1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We expect RNN with attention mechanism to have the greatest advantage when,\n",
    "\n",
    "1. The length of the input sentence is large.\n",
    "\n",
    "2. The length of the input sentence is short.\n",
    "\n",
    "Solution:__1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Quiz\n",
    "\n",
    "__1__ What is statistical machine translation ?\n",
    "\n",
    "    a.\n",
    "    \n",
    "    b.\n",
    "    \n",
    "    Solution: a\n",
    "    \n",
    "__2__ What MT method will you adopt if you have probablity distribution of input text word to output words.\n",
    "\n",
    "    a. Rule based MT\n",
    "    \n",
    "    b. Neural Network\n",
    "    \n",
    "    c. Statistical MT\n",
    "    \n",
    "    Solution: c\n",
    "    \n",
    "__3__ In beam search, if you increase the beam width B, which of the following would you expect to be true?\n",
    "\n",
    "    a. Beam search will run more slowly\n",
    "    \n",
    "    b. Beam search will use more memory\n",
    "    \n",
    "    c. Beam search will generally find better solutions(i.e, do a better job at maximizing P(y|x))\n",
    "    \n",
    "    d. Beam search will converge after fewer steps\n",
    "    \n",
    "    Solution: c\n",
    "    \n",
    "__4__ In machine translation, if we carry out beam search without using sentence normalization, the algorithm will tend to output overly short translations.\n",
    "\n",
    "    a. True\n",
    "    \n",
    "    b. False\n",
    "    \n",
    "    Solution: a\n",
    "    \n",
    "__5__ Consider using this encoder decoder model for MT\n",
    "\n",
    "<img src=\"../images/attention_model_quiz.png\"/>\n",
    "    \n",
    "\n",
    "This model is \"conditional language model\" in the sense that the encoder portion (shown in green) is modelling probablity of the input sentence x.\n",
    "\n",
    "    a. True\n",
    "    b. False\n",
    "    \n",
    "    Solution: b\n",
    "\n",
    "__6__ Compared to the encoder-decoder model shown above of this quiz (which does not use an attention mechanism), we expect the attention model to have the greatest advantage when:\n",
    "\n",
    "    a. The input sequence Tx is large\n",
    "    \n",
    "    b. The input sequence Tx is small\n",
    "    \n",
    "    Solution: a\n",
    "    \n",
    "__7__ What is a bleu score ?\n",
    "\n",
    "    a. Formula to calculate the accuracy of translation of MT models.\n",
    "    \n",
    "    b. Statistical score to calculate the parameter weight\n",
    "    \n",
    "    c. Score to evaluate the weight of the features.\n",
    "    \n",
    "    Solution: a\n",
    "\n",
    "\n",
    "__8__ Which RNN cell performs better in seq2seq Machine Translation ?\n",
    "\n",
    "    a. RNN\n",
    "    \n",
    "    b. LSTM\n",
    "    \n",
    "    c. GRU\n",
    "    \n",
    "    Solution: c\n",
    "    \n",
    "__9__ Would you use attention if the size of the input text is rougly 5 words.\n",
    "    \n",
    "    a. Yes\n",
    "    \n",
    "    b. No\n",
    "    \n",
    "    Solution: b\n",
    "\n",
    "    Explanation: No, seq2seq would suffice in such cases as attention only performs better if the number of input words are more.\n",
    "    \n",
    "__10__ How do you handle input text of variable length data ?\n",
    "\n",
    "    a. By ignoring the words outside of the threshold \n",
    "    \n",
    "    b. By having a a threshold equal to the maximum length of the input data and padding the remaining input.\n",
    "    \n",
    "    Ans: b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
