{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Machine Translation\n",
    "\n",
    "Machine translation (MT) is automated translation. It is the process by which computer software is used to translate a text from one natural language (such as English) to another (such as Spanish). \n",
    "\n",
    "Until very recently, if you wanted to know the Chinese translation of an English sentence, you had to hire a linguist who would both know English and Chinese language and would then translate the english text into chinese. Problem solved. But what if you wanted to translate the same piece of text into 5 different languages ?\n",
    "\n",
    "In recent times, MT has become one of the hottest areas of research given the success it is getting using deep learning. We would have all used Google Translate at some point or the other. Researchers are devising new techniques which are getting them higher accuracy in translating from one language to another. Years of R&D in deep neural networks have resulted in advances in the field of Machine Translation using which we could translate a given piece of text to any language.\n",
    "\n",
    "This will be the main focus of this tutrial where different aspects of Machine translation will be presented on along with hands on tutorial to implement it yourself. So let's get right on to it.\n",
    "\n",
    "## 1.1 Approaches\n",
    "\n",
    "Machine Translation(MT) has been an active area of studies right since the very beginnning of the computing age. Over the years it has evolved to what it is today. Some methods included pure semantic translation of the sentence / text based on some linguistic rules like replacing the words of the source language by the words of the target language. While, with the advent of latest search algorithms and big data, the process then shifted to Statistical based approaches. In the new era, with advances in Artificial Intelligence, MT has achived new heights and ever greater accuracy. Lets take a look at each of these processes in a bit more detail to better understand the journey of machine translation.\n",
    "\n",
    "   __1: Rule Based MT__: Rule based approach relies on countless built-in algorithms and sophisticated linguistic rules and millions of bi-lingual dictionaries of each language pair. The text would then be parsed and a transitional representation of the text would then be created, from which the final target text is generated. The translations are built on top of gigantic dictionaries having words from source and the target language thereby exponentially increasing the development cost and time. For example, let's say that we have to translate the following English text to German.\n",
    "   \n",
    "   __Source Language__ :English:  A girl eats an apple.\n",
    "   \n",
    "   __Target Language__: German\n",
    "   \n",
    "   According to Rule based MT, we need following to successfully translate the English text to German:\n",
    "   \n",
    "   1. A dictionary set, mapping each word in the English language to its German counterpart.\n",
    "   \n",
    "   2. Rules to represent English grammar\n",
    "   \n",
    "   3. Rules to represent German grammar.\n",
    "   \n",
    "   4. Rules relating English grammar to German grammar.\n",
    "   \n",
    "   \n",
    "   So using the above rules, we can attempt to translate the text to German in following stages:-\n",
    "   \n",
    "   __Stage 1__:\n",
    "      Get POS(Parts of speech) information.\n",
    "      \n",
    "      `a` article, `girl` noun, `eats` verb, `an` article, `apple` noun\n",
    "      \n",
    "   __Stage 2__:\n",
    "       \n",
    "      Then translate into German text by dictionary lookup and rules of grammar\n",
    "      \n",
    "      `a` = `ein`\n",
    "      \n",
    "      `girl` = `Madchen`\n",
    "      \n",
    "      `eat` = `essen`\n",
    "      \n",
    "      `apple` = `Apfel`\n",
    "      \n",
    "      So the translation would look something like :\n",
    "      \n",
    "      `A girl eats an apple` => `Ein Madchen isst einen Apfel`\n",
    "    \n",
    "   __2: Statistical MT__: Statistical MT tries to generate translations using statistical methods bases on bilingual text corpora. A document is converted according to probablity distribution $p(e|f)$ that a string $e$ in the target language, for example English is the translation of a string $f$ in the source language (for example, French). \n",
    "   \n",
    "   Typically in Statistical MT models, sentences are translated at once. A bilingual text corpus is trained and translated into probablity functions, like Bayes Theorem, where the translation model depicts a probablity function which estimates the probablity of a source string being a translation of the target string(from the dictionary).\n",
    "   \n",
    "   __Limitations__:\n",
    "   * The accuracy of this approach is directly proportional to the availability of the corpus of the language pair. The constraint here is the limited availability of such corpora for many language pairs thus limiting major success using the Statistical method.\n",
    "   \n",
    "   * Corpus training and creation can be really costly\n",
    "   \n",
    "   * Some errors can be hard to predict and fix.\n",
    "   \n",
    "   __Other statistical based approaches:__\n",
    "   \n",
    "   __2.1: Word based MT__: In word based MT, the fundamental unit of translation is a word. So each word of the source language is mapped on to one or more word of the target language. And then, a probablity function determines the best word(from the target language) to repressent the word from the source lang. However, the number of words in the translatd sentence can differ since a word in source language may tranlate to one or more words in the target language. To have a standard statistical measurement for this, a measure of ration of lengths of sequences of translated words called __fertility__ was introduced. It basically tells how many target language words, a source language word produces. It was assumed from the principles of Information theory that each of these words would cover the same concept. In practise however, this was not really true. For example, the English word corner can be translated in Spanish by either rincón or esquina, depending on whether it is to mean its internal or external angle.\n",
    "   \n",
    "   Let us try and understand this with a more concrete example. In what follows we will try to translate a German text into English.\n",
    "   Haus (German) -> house, building, home, household, shell(The house of a snail is shell)\n",
    "   \n",
    "   Now we see here that 1 german word has multiple english word translation based on the context of usage etc. Using statistical MT based approach, we will find out the word with the hightst probablity in the corpus(as shown in the figure below)\n",
    "   \n",
    "   <img src=\"../images/wordbasedmt.png\"/>\n",
    "\n",
    "\n",
    "Now when we look at the maximum likelihood score of the above words,\n",
    "\n",
    "<img src=\"../images/probablity_wordmt.png\"/>\n",
    "\n",
    "So we choose \"house\" as the probable translation to the word \"Haus\". Similarly we translate all the other words in source text (German) to target text (English):-\n",
    "\n",
    "<img src=\"../images/wt.png\"/>\n",
    "\n",
    "   __2.2: Phrase Based MT__: Phrase based models adopt a similar way of translation as Word based MT, the difference being, in Word based MT, the atomic unit is a word, whereas in Phrase based MT, the atomic unit is a phrase.\n",
    "Source text is split into phrases and then each phrase is then translated into the target language.  \n",
    "\n",
    "Let's try and understand this with an example. Let us consider the following source text(German):\n",
    "\n",
    "<img src=\"../images/phrasemt1.png\"/>\n",
    "\n",
    "As you can see in the figure above, the foreign text is segmented into phrases. Now similar to Word based translation, maximum likelihood score for the phrases will be calculated from the corpus and each of the probable phrases in the target language (English) will be assigned a score. The phrase with the best score would then be selected.\n",
    "\n",
    "For example, for the phrase translation of the word \"natuerlich\", the probablity scores could be something like :\n",
    "\n",
    "<img src=\"../images/phrasemt2.png\"/>\n",
    "\n",
    "Similarly the scores for each of the phrases would be calculated and then the translation to the target language would be the output as shown in the following:\n",
    "\n",
    "<img src=\"../images/phrasemt3.png\"/>\n",
    "\n",
    "   __3: Neural Machine Translation__: Neural MT takes advantage of the advances in deep learning in the world today and attempts to mitigate the shortcomings of other machine translation approaches that we studied earlier. It has been observed that it is able to produce better results with greator accuracy and with requirement of less data. Infact, many popular neural based machine translation do not need any inputs regarding grammar, rules of the language etc(unlike rule based / statistical machine translation).\n",
    "   \n",
    "Many of the popular neural MT approaches use a framework called Encoder-Decoder framework, in which the source language is encoded into an intermediatory machine generated language and is then decoded by another Decoder layer. This approach has proved to be very efficient in generating translations with very high accuracy and is currently the industry adopted approach for machine translation. We will be studying this is greator detail in the next few chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2:Neural Machine Translation\n",
    "\n",
    "## 2.1 Seq2Seq Modelling\n",
    "\n",
    "Seq2Seq models use neural networks to translate a piece of text from a source language to a machine language. Introduced by Google, it has proved itself for a variety of applications, namely, machine translation, image captioning, conversation models, text summarization etc. \n",
    "\n",
    "As the name sugests, it injests words in sequence and translates them. The beauty of seq2seq is that it not only considers the word, but it also considers the neighbouring words to capture better semantics. It uses RNN to learn and then translate an unseen piece of text from one language to another. More often than not, some specialized RNNs like LSTM or GRU is used since the vanilla RNN suffers from the problem of vanishing gradients. \n",
    "\n",
    "Seq2Seq models use a special arrangements of neural networks, viz, __Encoder Decoder__ networks to translate. The Encoder takes in the source text and outputs an intermediate vector which represents the input sequence. The decoder then reads the intermediate vector output of the encoder and then translates it into the target language. The following figure gives a pictorial representation of seq2seq model.\n",
    "\n",
    "<img src=\"../images/encodec_overview.png\"/>\n",
    "\n",
    "### Encoder\n",
    "\n",
    "1. A stack of several recurrent units (LSTM or GRU cells for better performance) where each accepts a single element of the input sequence, collects information for that element and propagates it forward.\n",
    "\n",
    "2. The input sequence is a collection of all words from the question. Each word is represented as x_i where i is the order of that word. The words are then encoded into dense vectors to represt the input text. This vector aims to encapsulate the information for all input elements in order to help the decoder make accurate predictions.It acts as the initial hidden state of the decoder part of the model.\n",
    "\n",
    "\n",
    "### Decoder\n",
    "\n",
    "1. A stack of several recurrent units where each predicts an output y_t at a time step t.\n",
    "\n",
    "2. Each recurrent unit accepts a hidden state from the previous unit(Encoder unit) and produces and output as well as its own hidden state.\n",
    "\n",
    "3. The output sequence is a collection of all words from the answer. Each word is represented as y_i where i is the order of that word.\n",
    "\n",
    "The power of this model lies in the fact that it can map sequences of different lengths to each other. As you can see the inputs and outputs are not correlated and their lengths can differ. This opens a whole new range of problems which can now be solved using such architecture.\n",
    "\n",
    "\n",
    "The Encoder - Decoder model not only gets better accuracy, but is also much scalable than any of the traditional machine translation methods know thus far. Infact, seq2seq is so popular and effective that any modern day machine translation happens via this. Google translate is one of the most popular platforms relying on seq2seq for translating text in different languages.\n",
    "\n",
    "### Uses in Industry\n",
    "\n",
    "Seq2seq has proved its mettle and is being used in lot of areas to solve complex problems. Let's take a look at some of these.\n",
    "\n",
    "#### Machine Translation : - \n",
    "\n",
    "Used to translate text from one language to another language. We will study this is more detail in later sections.\n",
    "\n",
    "#### Image Captioning :-\n",
    "\n",
    "Image Captioning is a technique, where in computer automatically generates captions for an image. For example if there is a picture in which a cat is sitting on a table, computer would generate a caption like \"Cat sitting on table\". \n",
    "\n",
    "For those of use with knowledge / interest in convulational neural networks, the image is converted into vectors using conv-nets, and then the resulting vector is trained on seq2seq models. The same can be envisioned using the image below:-\n",
    "\n",
    "<img src=\"../images/img_captioning.jpg\"/>\n",
    "\n",
    "#### Other uses\n",
    "\n",
    "Seq2seq finds additional uses in :-\n",
    "\n",
    "    *Conversation models (for chatbots etc)\n",
    "    \n",
    "    *Document sumarization\n",
    "    \n",
    "    \n",
    "    \n",
    "### Seq2Seq in action\n",
    "\n",
    "Having seen how seq2seq works at a very high level, let us try and understand how do we actually translate a text from a source language to a target language. \n",
    "\n",
    "For our study let us consider the following sentence in Spanish :\n",
    "\n",
    "__Patrick visitará la India en septiembre.__\n",
    "\n",
    "The task at hand is to translate this sentence into English. However we do have some problems to solve first.\n",
    "\n",
    "1. To translate the text correctly into the target language.\n",
    "\n",
    "2. To check if the translated text is correct.\n",
    "\n",
    "3. To select the best available translation(after 1 & 2) of the target language.\n",
    "\n",
    "Point 1 is pretty clear in itself. However, 2 brings with itself additional problems,i.e, how do we verify if the translated text is correct. Point 3 futher states that, assuming if the translation(s) are correct, which one do we select. How do we determine the accuracy of the translation. Forexample, let us look at the following translations of the above text:\n",
    "\n",
    "1. Patrick will visit India in September.\n",
    "\n",
    "2. Patrick is going to visit India in September.\n",
    "\n",
    "3. In September, Patrick will be  visiting India.\n",
    "\n",
    "4. In September, Patrick is welcome in India.\n",
    "\n",
    "Translations from 1-3 are more or less correct and they kind of convey the meaning of the source language. However, translation number 1 is the best out of all the 4 because it conveys the meaning of the source text precisely and concisely and also is grammatically correct. However, translation 4 goes awfully wrong. We will see how tackle / solve the above problems one by one.\n",
    "\n",
    "Let us try and define the objective now. The objective of a machine translation model is to translate a given piece of text in one language to its best possible match in the target language. Or in other words, find a piece of text in the target language whose probablity of being a correct translation of the piece of text in the source language is the highest. Mathematically, we can say that we have to find translation(text in target language) which has the highest conditional probablity of being the correct translation given the source text. In another words we need to find the value of $Y_i$ which maximizes $P(Y|X)$ ,where $Y$ is a set of all the probable candidates for the correct translation and $X$ is the source text. To find the words from the target dictionary, we use a specialized algorithm called Beam Search. \n",
    "\n",
    "__Beam Search__\n",
    "\n",
    "In order to translate successfully, we need to find the words to begin with. Beam search is a specially designed algorithm to help us with that. It will find the most probable word(s) from the dictionary of the target language that will be used in the translated text. Let's take a look at the image below:-\n",
    "\n",
    "<img src=\"../images/beamsearch_step1.png\"/>\n",
    "\n",
    "\n",
    "Following are the actions performed by the beam search algorithm to find out the words from the target language.\n",
    "\n",
    "__Step 1__:\n",
    "\n",
    "1. Given dictionary of the target language & the beam width (explained in the figure), find out the probablity of the all the words from the dictionary given the source text. \n",
    "\n",
    "    $P(Y_i|X)$ , where $Y_i$ are the words from the dictionary and $X$ is the source text to be translated.\n",
    "    \n",
    "    So essentially the beam search algorithm would take all the words in the dictionary of the target language and run it through a softmax layer over an encoder-decoder network, essentially calculating conditinal probability of all the words in the dictionary so as to choose the best first word.\n",
    "    \n",
    "    <img src=\"../images/beamsearch2.png\"/>\n",
    "\n",
    "2. Select top n = beam width words which have the highest conditional probablity. In this case let us say the top three words that the beam search algorithm picks up is \"patrick\", \"in\", \"september\".\n",
    "\n",
    "\n",
    "__Step 2__:\n",
    "\n",
    "1. After getting the most probable first words for the translation, the beam search algorithms needs to find the next words. To do that,\n",
    "    * It keeps the first words in memory(number of words equal to the max beam length) calculates the probablity of the second word given the first word and the input sentence. An intuition can be developed from the image below:-\n",
    "    <img src=\"../images/beamsearch_s20.png\"/>\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "<img src=\"../images/beamsearch_step2.png\"/>\n",
    "\n",
    "\n",
    "As we can see from the image above, the algorithm now tries to find out the second word keeping the first set of words fixed (from step 1). Mathematically, in this step we are trying to maximize the probablity of two words given the input sentence.\n",
    "$P(Y_1,Y_2|X)$\n",
    "\n",
    "By the rules of conditional probablity, this can also be written as:-\n",
    "\n",
    "$P(Y_1,Y_2|X)=P(Y_1|X)*P(Y_2|X)$\n",
    "\n",
    "When the above operation is carried out on all the words of the dictionary one by one, we get the pair of words with the highest probablity as probable candidates for the translation.\n",
    "\n",
    "The above equation can also be extented to n terms, for example\n",
    "\n",
    "$P(Y_1...Y_n|X)=P(Y_1|X)*P(Y_2|X)...P(Y_n|X)$\n",
    "\n",
    "The above steps are repeated until all the probable words for the translation are found out by the algorithm. So the entire process when combined might look something like this:-\n",
    "\n",
    "<img src=\"../images/machine_translation.png\"/>\n",
    "\n",
    "\n",
    "#### Translation Evaluation: Bleu Score (Bilingual evaluation understudy)\n",
    "\n",
    "Now, once we have a proper translation, how do we verify the results? To evaluate a MT model, the standard way is to calculate the Blue score. However, The standard way to calculate the accuracy of a ML model is by calculating __Precision__. Let's see why calculating Precision will not work in our example. So let's take our translation example:-\n",
    "\n",
    "Spanish : Patrick visitará la India en septiembre.\n",
    "\n",
    "Some human verified references :\n",
    "\n",
    "1. Patrick will visit India in September.\n",
    "\n",
    "2. Patrick is going to visit India in September\n",
    "\n",
    "\n",
    "Now, let's take a look at what a machine translated output could look like in extreme cases :-\n",
    "\n",
    "1. Patrick will visit India in September.\n",
    "\n",
    "2. visit visit visit visit visit visit\n",
    "\n",
    "Now if we were to calculate the Precision of the above outputs, it would give some unexpected results. Like, let's calculate the precision of machine translated output #2 \n",
    "\n",
    "Recall that precision is number of correct predections (In this case a prediction is correct is the word is present in the human reference ) divided by total correct predictions. So in our reference num 1 , we see there are 6 words, and from the machine translated output, we see that all the 6 words (visit) is present in the human reference. So this essentially gives us a precision of 1 for a poorly translated output.\n",
    "\n",
    "It is for this problem, a new measure for calculating the effectiveness of machine translation was invented.\n",
    "\n",
    "Blue score is calculated in the following way:-\n",
    "\n",
    "1. n-Grams are considered for calculating the score.\n",
    "\n",
    "2. Number of occurance of n-Grams(in the above example, unigrams ) is capped to the maximum number of occurance of the n-Grams in any of the references.\n",
    "\n",
    "\n",
    "Having gained insight into how seq2seq model works, let's try and implement it in python in the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Machine Translation in Python\n",
    "\n",
    "In the last chapter we learnt about Seq2Seq modelling for machine translation. In this chapter we will actually learn how to implement a machine learning model in Python. We will learn how to use translate sentence from a source language to another language by building a Seq2Seq model using Keras, a popular python deep learning API. \n",
    "\n",
    "Here is what we are going to do:\n",
    "\n",
    "1. Turn each of the sentence into 3 Numpy arrays, __encoder_input_data, decoder_input_data, decoder_target_data__:\n",
    "\n",
    "__encoder_input_data__ is a 3D array of shape (num_pairs, max_jap_sentence_length, num_jap_characters) containing a one-hot vectorization of the Japanese sentences.\n",
    "\n",
    "__decoder_input_data__ is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
    "\n",
    "__decoder_target_data__ is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n",
    "\n",
    "2. Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data. Our model uses teacher forcing.\n",
    "\n",
    "3. Decode some sentences to check that the model is working (i.e. turn samples from encoder_input_data into corresponding samples from decoder_target_data).\n",
    "\n",
    "\n",
    "Because the training process and inference process (decoding sentences) are quite different, we use different models for both, i.e, they all leverage the same inner layers.\n",
    "\n",
    "This is our training model. It leverages three key features of Keras RNNs:\n",
    "\n",
    "1. The return_state contructor argument, configuring a RNN layer to return a list where the first entry is the outputs and the next entries are the internal RNN states. This is used to recover the states of the encoder.\n",
    "\n",
    "2. The inital_state call argument, specifying the initial state(s) of a RNN. This is used to pass the encoder states to the decoder as initial states.\n",
    "\n",
    "3. The return_sequences constructor argument, configuring a RNN to return its full sequence of outputs (instead of just the last output, which the defaults behavior). This is used in the decoder.\n",
    "\n",
    "Knowing the overview of we are trying to achieve, let's understand the code snippets that we can use the build the Keras  model.\n",
    "\n",
    "\n",
    "First and formost, we import the libraries that we will be needing. Following snippet does that:-\n",
    "\n",
    "```python\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "```\n",
    "\n",
    "In the snippet below, we define the Encoder Inputs as a Keras Input Layer. The keras input layer is passed on parameters about the shape of the input and the number of word tokens in the input sequence.\n",
    "\n",
    "We also define an LSTM model `encoder`.\n",
    "\n",
    "We then call the encoder function to get the encoder output and the hidden state variables. We discard encoder_output and keep the state variables, namely `state_h` and `state_c` for further processing.\n",
    "```python\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "```\n",
    "\n",
    "We will not setup the decoder network to decode from the encoder states. Following are the things that we do in this phase:\n",
    "\n",
    "1. Set up the decoder, using `encoder_states` as initial state.\n",
    "\n",
    "2. We set up our decoder to return full output sequences,and to return internal states as well. We don't use the \n",
    "return states in the training model, but we will use them in inference.\n",
    "\n",
    "Following snippet shows the process in more detail.\n",
    "\n",
    "``` python\n",
    "\n",
    "# Setup Decoder network\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "```\n",
    "\n",
    "Once we have the decoder layer setup, we now define a Keras model that takes in the `encoder_input_data` and `decoder_inputs` and outputs the `decoder_target_data`.\n",
    "\n",
    "```python\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "```\n",
    "\n",
    "Once we have the model, we run training on the model to train it on the input Japanese data. Following snippet does that:-\n",
    "\n",
    "\n",
    "``` python\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "```\n",
    "\n",
    "Once we have the trained model, we will use this model to decode the sentence and print out the output. \n",
    "\n",
    "Hope the above section of implementing a machine translation using python brings some clarity on how can you implement your own machine translation system. In the section below, we will now actually implement the above concept on a sample dataset. The dataset contains English - French translation. On the lines described above, let's start to build an encoder decoder model using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "# Setting training environments and other variables\n",
    "train_batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim=256\n",
    "num_samples=10000\n",
    "\n",
    "\n",
    "\n",
    "# Explore the dataset a bit, aye ?\n",
    "df = pd.read_csv(\"../data/frenchenglish-bilingual-pairs/fra-eng/fra.txt\",delimiter='\\t')\n",
    "df.head(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataset is a tad delimited file of Eglish - French translation. Let us now prepare this dataset to suit the parameters required to use Keras apis as described in the section above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_chars=set()\n",
    "target_chars=set()\n",
    "lines = open(\"../data/frenchenglish-bilingual-pairs/fra-eng/fra.txt\",encoding='utf-8').read().split(\"\\n\")\n",
    "for line in lines:\n",
    "    try:\n",
    "        input_text, target_text= line.split(\"\\t\")\n",
    "        target_text = '\\t' + target_text + '\\n'\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "\n",
    "        for char in input_text:\n",
    "            if char not in input_chars:\n",
    "                input_chars.add(char)\n",
    "\n",
    "        # Create a set of all unique output characters\n",
    "        for char in target_text:\n",
    "            if char not in target_chars:\n",
    "                target_chars.add(char)\n",
    "\n",
    "       \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "input_chars = sorted(list(input_chars))\n",
    "target_chars = sorted(list(target_chars))\n",
    "num_encoder_tokens = len(input_chars) # aka size of the english alphabet + numbers, signs, etc.\n",
    "num_decoder_tokens = len(target_chars) # aka size of the french alphabet + numbers, signs, etc.\n",
    "\n",
    "                      \n",
    "input_token_index = {char: i for i, char in enumerate(input_chars)}\n",
    "target_token_index = {char: i for i, char in enumerate(target_chars)}\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) # Get longest sequences length\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "\n",
    "# encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) \n",
    "# containing a one-hot vectorization of the English sentences.\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) \n",
    "# containg a one-hot vectorization of the French sentences.\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_target_data is the same as decoder_input_data but offset by one timestep. \n",
    "# decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :]\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "# Loop over input texts\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # Loop over each char in an input text\n",
    "    for t, char in enumerate(input_text):\n",
    "        # Create one hot encoding by setting the index to 1\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    # Loop over each char in the output text\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "print(\"Done preparing data for training\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once we have the dataset prepared as per the requirement of Keras APIs, we will now train the model as per the explanation above:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name = 'encoder_inputs')\n",
    "\n",
    "# The return_state contructor argument, configuring a RNN layer to return a list \n",
    "# where the first entry is the outputs and the next entries are the internal RNN states. \n",
    "# This is used to recover the states of the encoder.\n",
    "encoder = LSTM(latent_dim, \n",
    "                    return_state=True, \n",
    "                    name = 'encoder')\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), \n",
    "                       name = 'decoder_inputs')\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, \n",
    "                         return_sequences=True, \n",
    "                         return_state=True, \n",
    "                         name = 'decoder_lstm')\n",
    "\n",
    "# The inital_state call argument, specifying the initial state(s) of a RNN. \n",
    "# This is used to pass the encoder states to the decoder as initial states.\n",
    "# Basically making the first memory of the decoder the encoded semantics\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, \n",
    "                      activation='softmax', \n",
    "                      name = 'decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "                    decoder_target_data,\n",
    "                    batch_size=1000,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "# Save model\n",
    "#model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the inference mode. In this we do the following:\n",
    "\n",
    "1. Encode input and retrieve initial decoder state\n",
    "\n",
    "2. Run 1 step of decoder with this initial state and a start sequence as target. The output of this will be the next target token\n",
    "\n",
    "3. Repeat the current target token and the current states.\n",
    "\n",
    "Let's see how to implement this in the following section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "#define decoder initial states\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "#define decoder output seq and model\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the inference model ready,  we gotta write a function that will convert the decoded output (which is basically location of the words) to human readable translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take our model for a spin and translate some of the English sentences to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Attention Model\n",
    "\n",
    "In the last chapter, we learnt about Machine Translation and how to do it in Python. In this chapter, we will learn another very important concept called Attention Mechanism. Attention mechanisms have gained much popularity in recent times because of using them has increased the accuracy of the Encoder-Decoder RNN model.\n",
    "\n",
    "### 3.1 Introduction of Attention Model\n",
    "\n",
    "__What is Attention Model?__\n",
    "\n",
    "Well, attention model was majorly proposed as a solution to the traditional Encoder-Decoder model in machine translation. Recall that the Encoder-Decoder model used to encode a fixed length vector injesting all the input data at once to feed it to the Decoder which would then decode all the input (output of the decoder) at once.\n",
    "\n",
    "__Why Attention Models?__\n",
    "\n",
    "Now compare it to a typical problem of machine translation. Imagine if you or any human were to translate the a given input sentence to a target output language. If you were reading the complete sentence at once and translating it to the target language, you would probably do good for short sentences, like < 10 words, but would you be efficient if the input sentences were long ? No, right. Ironically the same behaviour was observed when traditional machine translation models were used in translation. It was found that the accuracy of the model was okay till about a certain length of the input sentence. However, when the length of the input sentence increased, the accuracy of the MT models started going down.\n",
    "\n",
    "This could be explained from the following figure:-\n",
    "\n",
    "<img src=\"../images/attention_1.png\"/>\n",
    "\n",
    "__Advantages__\n",
    "\n",
    "Attention models address this exact problem faced in traditional MT models. Let's try to understand this with the human translating sentence example we saw earlier. Now typically when a human gets a long input sentence, when he/she would sub-consciously do is break the sentence into parts, like of 4-5 words, read them and translate it into target language, and then move on to the next chunk of sentence. The way these small chunks are decided completely depends on person to person, but the idea is that a large sentence is then broken down into smaller sentence and then each of the smaller sentence is translated. It was observed that using this approach, the MT accuracy was high, even for sentences with large number of words.\n",
    "\n",
    "<img src=\"../images/attention_2.png\"/>\n",
    "\n",
    "### 3.2 How does it work ?\n",
    "\n",
    "In this section let's try to understand how does attention model work. How is it able to maintain accuracy of the prediction over a large input sentence. To do that lets take a look at the following figure, which extends to our last learnt concept of encoder-decoder networks.\n",
    "\n",
    "<img src=\"../images/attention_3.png\"/>\n",
    "\n",
    "The way the Attention Model differs from typical Encoder Decoder model is that instead of the decoder network directly decoding the output of the Encoder network, it is not passed on a Context vector $C$ which is a combination of output of the Encoder network $\\alpha$ and an attention parameter $A$. The Context vector $C$ is then passed on to the decoder network to get the final output. Let's try to understand this process in more detail.\n",
    "\n",
    "1. As you can see in the figure above, attention parameter is calculated for each of the input sequences. We denote this attention parameter using $\\alpha$. $\\alpha_i$ denotes the amount of attention that you need to pay to its corresponding input vector $x_i$. To get an intuition, think of it as a complex function which determines the weight of the input sequence. \n",
    "The way you calculate $\\alpha_i$ is to train a small neural network which basically takes in two parameters:-\n",
    "\n",
    "    1. The output of the encoder network of a particular input sequence.\n",
    "    \n",
    "    2. The hidden state of the encoder network (which we were not considering at all in the traditional Encoder- Decoder) network.\n",
    "    \n",
    "    The following figure demonstrates the above concept. \n",
    "    \n",
    "    <img src=\"../images/attentionparam.png\"/>\n",
    "\n",
    "2. Once you have the attention parameter $\\alpha$, we now compute a Context vector which is nothing but a weighted sum of the product of:\n",
    "    1. Output of the encoder netework.\n",
    "    \n",
    "    2. Attention parameter.\n",
    "    \n",
    "    $C=\\sum_{i=1}^n\\alpha_i*A_i$\n",
    "    \n",
    "3. The Context vector $C$ is then fed into the deocder network to predict the output. To predict the output, we use a feed forward neural network, which takes in the Context Vectors (from step 2 ) as the parameters and then predicts the decoded output of the network. The following figures attempts to give an intuition on the same.\n",
    "\n",
    "<img src = \"../images/decoder_attention.png\"/>\n",
    "\n",
    "Notice here that the input to the decoder is now an attention parameter which indicates how much of attention to give to each of the words in the input sequence and the output of the encoder. This way the decoder network is able to decode the input sequence in batches (since the attention parameter tends to zero for far off words. Which makes sense also since usuallly the far off words do not carry the context or any relation with the current word in a language.)\n",
    "\n",
    "Now let's look at how do we implement attention model using tensorflow in the following section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-French dataset like in the previous chapter. Here is a breif description of the process that we are going to follow to prepare the dataset.\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "\n",
    "2. Clean the sentences by removing special characters.\n",
    "\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "\n",
    "4. Pad each sentence to a maximum length.\n",
    "\n",
    "As we saw in the last tutorial let's load the dataset for processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "\n",
    "path_to_file=\"../data/frenchenglish-bilingual-pairs/fra-eng/fra.txt\"\n",
    "df = pd.read_csv(\"../data/frenchenglish-bilingual-pairs/fra-eng/fra.txt\",delimiter='\\t')\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "# We will now preprocess the dataset to be used to Model Consumption.\n",
    "\n",
    "## The following class will create a dictionary of words for the dataset. \n",
    "## The dictionary will be in the form of ID-> WORD structure. Forexample, \"mom\"->7\n",
    "\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "    \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "            \n",
    "## Load the dataset in proper format\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "    targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # French sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    # English sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n",
    "\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now write our Encoder Decoder model. Also for this chapter we will be using GRU instead of LSTM for simplicity since GRU has just one state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let up define GRU units for calculation\n",
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define a loss function to train our Encoder\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this brings us to the conclusion of the chapter on Attention mechanism. Let us try and answer the following questions to test our undersanding on Attention MEchanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As a datascientist when using machine translation algorithms, you find that the accuracy of your model is decreasing and also the length of your input sentences is increasing. What would you do ?\n",
    "\n",
    "1. Use recursive units instead of recurrent\n",
    "\n",
    "2. Use attention mechanism\n",
    "\n",
    "3. Use character level translation\n",
    "\n",
    "4. None of these\n",
    "\n",
    "Solution: __2__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The network learns to pay attention by learning the values of Context vector. Can we train a small NN to get the context vectors?\n",
    "\n",
    "1. True\n",
    "\n",
    "2. False\n",
    "\n",
    "Solution:__1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We expect RNN with attention mechanism to have the greatest advantage when,\n",
    "\n",
    "1. The length of the input sentence is large.\n",
    "\n",
    "2. The length of the input sentence is short.\n",
    "\n",
    "Solution:__1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
