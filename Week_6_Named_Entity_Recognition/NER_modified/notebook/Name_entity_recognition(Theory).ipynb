{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction and Named Entity Recognition\n",
    "\n",
    "## Overview\n",
    "In this tutorial you will learn about extracting information from unstructured text. This is an important step which lets us make sense of the raw data which we often get in the real world.\n",
    "We will learn about the following in this tutorial:\n",
    "* Information Extraction and various techniques\n",
    "* Regular Expressions and how to use them in Python\n",
    "* Named Entity Recognition\n",
    "* Condition Random Fields (A popular algorithms used in various NER systems.)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this tutorial, you will be able to do the following:\n",
    "    * Gain hands-on knowledge on using Regular Expressions to extract intended data from unstructured text.\n",
    "    * Know how to extract Named Entities from raw text\n",
    "    * Use regex in Python\n",
    "    * Explore a popular algorithm behind NER systems.\n",
    "    \n",
    "## Pre-requisites\n",
    "\n",
    "After completing this tutorial, you will be able to do the following:\n",
    "    * Gain hands-on knowledge on using Regular Expressions to extract intended data from unstructured text.\n",
    "    * Know how to extract Named Entities from raw text\n",
    "    * Use regex in Python\n",
    "    * Explore a popular algorithm behind NER systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction\n",
    "\n",
    "### 1.1 Why do we need IE?\n",
    "\n",
    "\n",
    "This tutorial is aimed at providing knowledge about __Information Extraction(IE)__ or __Information Retrieval(IR)__, the whats', the hows' and the whys'. IE is the study of techniques used to extract necessary information from raw data. IE is vast, and one of the most important field of study in data sciences, both theoritically and practically. It has been the interest and an active area of work for researchers for decades. Cleaning data, extracting necessary data to form features from vast amount of unstructured data is also what constitutes for 50-60 % of time spent by a data scientist on a real project. \n",
    "\n",
    "\n",
    "Typically in real life scenrios, applications generate lot of data. Most part of this data may / may not be needed for data science algorithms. So IE forms a bridge between the unstructured, raw data to the strructured formatted numerical data needed by various machine learning algorithms to perform their task. The flow diagram below gives a high level view of where IE systems play their part in the big picture.\n",
    "\n",
    "<img src=\"../images/ie_flowdiag.png\"/>\n",
    "\n",
    "### Impact of IE/IR\n",
    "\n",
    "* With greater advancements in IE systems and techniques, we have garnered better capability to process the data and with greater accuracy. \n",
    "\n",
    "* Lot of big data technologies arose from the fact that we needed to process vast amount of data before/after feeding into actual business logic code. \n",
    "\n",
    "* The accuracy of machine learning algorithms have increased because of increase in the accuracy of feature set acquired from raw text.\n",
    "\n",
    "### Techniques / Approaches\n",
    "\n",
    "Following are the major techniques used to extract information from text:-\n",
    "\n",
    "* __Regular Expresions__ : Simple regular expressions used to find the pattern in the text and extract matching information.\n",
    "* __Classification Algorithms__ : Classification algorithms are also being used to extract out a subset of text of interest.\n",
    "* __Deep Learning Systems__ : Recurrent Neural Netwoks (RNN) are being used to extract out information of interest from raw text and have impressive accuracy in extracting information.\n",
    "* __Probablistic Models__ : Probablistic Models like Condition Random Fields (CRF) are gaining popularity for their efficiency in capturing sequential information.\n",
    "\n",
    "\n",
    "We are going to take up Regular Expressions and study it in detail.\n",
    "\n",
    "### What is Regular Expression ?\n",
    "\n",
    "A regular expression is a collection of one of more characters that define a pattern. Usually this represents a string. As an intuition it is an abstract representation of a string. Regular expression is a very powerful tool which can be used in searching a string or all the occurances of a string in a text,extract information from a piece of text.\n",
    "Following is a very basic representation of a regular expresion(infact the most basic one):-\n",
    "\n",
    "Regular Expression -> sh\n",
    "\n",
    "When we use the above regular expression, it basically says that find all the occurances in a text which contains 's' and 'h' in that order. Let's see an example in the text below\n",
    "\n",
    "Text -> She sells sea <mark>sh</mark>ells on the sea <mark>sh</mark>ore.\n",
    "\n",
    "** The point to note here is that the first occurance of the regex(Regular Expressions in short) is She, but is entirely ignored. It is because regexes are case sensitive. So a regex 'sh' would mean to select all occurances of the string sh and in lower case.\n",
    "\n",
    "### Okay, I am getting a hang of it now. Tell me more...\n",
    "\n",
    "Regular expressions are broadly a collection of characters, called metacharacters, and a condition specifying their repetition, called multipliers. Let's try and understand them in a bit more detail.\n",
    "*  __Metacharacter__ - These are regular expression specific characters which basically outline the selection criterion for the string.\n",
    "\n",
    "We will be seeing some of the metacharacters used in regex in the tutorial that follows.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __1. .(dot)__ - The dot is a metacharacter which represents any character(much like the joker in the game of cards). The important thing to note here is that the dot will represent only a ** single ** character. \n",
    "\n",
    "For Example:\n",
    "\n",
    "Sentence -> She sells sea-shells on the sea-shore. \n",
    "\n",
    ">In the sentence, there are 38 matches to the regex .(dot), since each character (including the white-space) is a match.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __2. [] (Brackets)__ - The brackets specify a range of characters to be matched. It fine tunes the dot(.), which basically selects any character, to a predifined set of characters to be matched. \n",
    ">Sample Usage-:\n",
    "[a-e], select all the text which has a,b,c,d,e <br/>\n",
    "[1-8], select any occurance of 1,2,3,4,5,6,7,8<br/>\n",
    "For Example:\n",
    "<br/>\n",
    "Regex -> [1-9] will select a single digit in the sentence\n",
    "<br/>\n",
    "Sentence -> A leap year comes after <mark>4</mark> years.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __3. ^ (Negation)__ - The negation expression lets us find the all the characters which do not match a given expression.\n",
    "\n",
    ">For Example:\n",
    "<br/>\n",
    "Regex -> [^1-9] will select all the characters except the digits in the range 1-9.\n",
    "<br/>\n",
    "Sentence -> <mark> A leap year comes after</mark> 4 <mark> years</mark>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __4. \\ (Escape Character)__ - Escape characters are the ones which let us escape the metacharacters of regular expression.\n",
    "\n",
    ">Regex -> \\..* will select all the characters after the '.'(dot). Please note that we escaped the '.'(dot) so that the regex engine does not mistake it to be a metacharacter and consider it as a dot in the sentence.\n",
    "<br/>\n",
    "Sentence -> Hi<mark>. How are you ?</mark>\n",
    "\n",
    "\n",
    "\n",
    "* __Multipliers__ - These are conditional loops which specify the number of times a find operation should be performed. When used in conjunction with an expression, it indicates repetition of that expression, 0 or more number of times.\n",
    "Following are the multipliers in the regular expressions.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __1. * (asterix)__ - This means that the expression occurs zero of more number of times. Example:\n",
    "\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Taking the above example, the regex \\..* uses '*' multiplier to suggest that select any number of characters that come after dot.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __2. + (plus)__ - Expression occurs one of more number of times.\n",
    "\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Continuing the previous example, if we modify the regex to '\\..+', it would mean select dot and one or more \n",
    "characters after dot. Please note that there will be no match in case there are no characters after dot. For example,<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Sentence-> Hi.<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Regex -> '\\ ..*' , There will be a match. . will be selected, since the regex does not mandate any character to be \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "present after dot.<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Regex -> '\\ ..+' , There will be no match, since there are no characters after not.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __3. ? (question mark)__ - Expression occurs **zero or 1 time**\n",
    "\n",
    ">Regex- s.?a<br/>\n",
    "Sentence-> She sells <mark>sea</mark> shells on the <mark>sea</mark> shore\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __4. {3}__ - Expression occurs ** three ** times\n",
    "\n",
    ">Regex- s.{3}ls<br/>\n",
    "Sentence-> She sells sea <mark>shells</mark> on the sea shore\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __5. {3,5}__ - Expression occurs between 3 and 5 times. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __6. {3,}__ - Expression occurs **atleast** three times. This means that a minimum of 3 repetitions will occur of the expression. There is no cap of the maximum number of times the expression will occur.\n",
    "\n",
    ">Regex- (sea){2,}<br/>\n",
    "Sentence - She sells sea shells on the <mark>seasea</mark> shore\n",
    "\n",
    "#### Some simple Regular Expression\n",
    "\n",
    "Let us take a look at some simple regular expression and try to understand how it really works.\n",
    "\n",
    "* Regex- **a.p**\n",
    "\n",
    "    The above regular expression will try to find three characters, a, followed by an character from a-z, followed by p\n",
    "\n",
    "    Text- The <mark>app</mark>le of my eye\n",
    "    \n",
    "* Regex **a.p.***\n",
    "\n",
    "    The above regex will try to find three or more character. It will do the following:\n",
    "        * Find a sequence that starts with a\n",
    "        * Then accept any character(the dot)\n",
    "        * Then match p\n",
    "        * then match 0 or more instances of any character.\n",
    "        \n",
    "    Let us apply this regex in the above example\n",
    "    \n",
    "    Text - The <mark>apple of my eye</mark>\n",
    "    \n",
    "* Let's up things a bit. Now we'll try to find out an email address from a piece of text( Did someone say data scraping ? ).\n",
    "\n",
    "   Regex - **[a-z]+@.+\\..{2,10}**\n",
    "   \n",
    "   Here is what the above regex means:-\n",
    "       * Find one or more characters in the range a-z the until you get '@'\n",
    "       * Then find one of more character, untill you find '.'\n",
    "       * Then find a maximum of ten characters after you find . (assuming the longest tld available is .technology which is 10 characters)\n",
    "        \n",
    "    Text -> Please write to use at <mark>info@grayatom.com</mark>\n",
    "    \n",
    "    The above regex is used for simplicity and there are lot of cases that get missed by it. For example, since we have already learnt that the regexes are case sensitive, so the above will miss out on capitalized words. Also we do encounter numbers in emails which would again get missed by the regex. We leave it upto the student to revise the regex to cover all the cases as an exercise.\n",
    "    \n",
    "### 1.2 Regular Expression in Python\n",
    "\n",
    "Regular Expressions in Python are supported by a popular package called 're'. This package contains all the necessary codebase related to regular expressions. Let us dive into code and try and understand how to use regular expressions to find desired information.\n",
    "\n",
    "```python\n",
    "\n",
    "# Import the re package\n",
    "import re\n",
    "\n",
    "# Let us use the same text as mentioned in the example above\n",
    "text = \"The apple of my eye\"\n",
    "\n",
    "# The following snippet will use the search function of re package. It will take two parameters:\n",
    "# 1. The first parameter will be the regular expression to search for\n",
    "# 2. The second parameter is the text in which the pattern is to be searched\n",
    "matched = re.search('a.p',text)\n",
    "\n",
    "'''\n",
    "    If the pattern is found:-\n",
    "        1. The match variable gets a True(boolean) value\n",
    "        2. Otherwise, it returns None\n",
    "'''\n",
    "\n",
    "if matched:\n",
    "    print(\"Pattern Found\")\n",
    "```\n",
    "#### Output\n",
    "\n",
    "```\n",
    "pattern Found\n",
    "\n",
    "```\n",
    "#### RE package in more detail\n",
    "\n",
    "The re package mainly provides 3 operations based on regular expressions. \n",
    "\n",
    "1. Match - Checks for the pattern only at the start of the string.\n",
    "2. Search - Checks for the pattern anywhere in the string\n",
    "3. Search and Replace - As the name suggests, it will search for a given pattern and replace all the occurances with the string provided. The search and replace function additionally takes a max parameter, which specifies  maximum number of occurances to replace.\n",
    "\n",
    "The overall structure of match and search function / method looks like this.\n",
    "\n",
    "|pattern|string|flags|\n",
    "|-------|------|-----|\n",
    "|Regex pattern|String to search|Modifier flags\n",
    "\n",
    "__Group Functions:__\n",
    "\n",
    "Group functions, when used on top of match / search will return the entire matched strings in form of an array(tuple) or a single match (if given the group number.)\n",
    "\n",
    "__Modifier Flags:__\n",
    "\n",
    "Following are some of the modifier flags. The reader is encouraged to read about all of them.\n",
    "\n",
    "|Flag|Description|\n",
    "|----|-----------|\n",
    "|I/IGNORECASE|Perform case-insensitive matching|\n",
    "|L/LOCALE|Interpret words as per the current locale|\n",
    "|M/MULTILINE|When specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline); and the pattern character '$' matches at the end of the string and at the end of each line (immediately preceding each newline)|\n",
    "|S/DOTALL|Make the '.' special character match any character at all, including a newline; without this flag, '.' will match anything except a newline. Corresponds to the inline flag (?s).|\n",
    "\n",
    "\n",
    "Now lets dive into some more examples and see how we can leverage the various re functions.\n",
    "\n",
    "\n",
    "```python\n",
    "# Import re package\n",
    "import re\n",
    "#Sample text\n",
    "text = \"She sells sea-shells on the sea shore.\"\n",
    "\n",
    "# Match 'sh' in the given text. Do not consider Case while matching \n",
    "match = re.match(r'sh',text,flags=re.IGNORECASE)\n",
    "# Prints the value of match. Rather inconclusive and not helpful if not grouped.\n",
    "print(match)\n",
    "# Match in itself does not give much information. As can be seen by printing the variable.\n",
    "#Junk value like '<_sre.SRE_Match object; span=(0, 2), match='Sh'>' gets printed\n",
    "\n",
    "# $$$$$ Groups to the rescue\n",
    "match = re.match(r'sh',text,flags=re.IGNORECASE).group()\n",
    "\n",
    "# Prints the matched expression\n",
    "print(match)\n",
    "\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "<_sre.SRE_Match object; span=(0, 2), match='Sh'>\n",
    "Sh\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Now let us up things a bit. Let us take the email example we discussed earlier.\n",
    "\n",
    "text = \"Please write to use at info@greyatom.com\"\n",
    "\n",
    "# We will be using the search function since we expect the email to be anywhere and not just at the start\n",
    "match = re.search(r'[a-z]+@.+..{2,10}',text).group(0)\n",
    "\n",
    "# The group function will find all the groups of occurances. We pick the first occurance\n",
    "print(match)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:\n",
    "\n",
    "### Develop a fully functional Email Regex\n",
    "The above email regex does not cover all the cases. The reader is expected to complete the following exercise and figure out a regex that covers all the cases generally found in an email.\n",
    "__Hint:__ The above regex will fail to collect the email info1@grayatom.com\n",
    "\n",
    "#### Instructions\n",
    "* Identify all possible use cases and write a regex that covers all the email types\n",
    "* Replace the empty regex, with your regex. Please do not modify any other lines\n",
    "* Run the cell, If your regex is right, a success message will be printed. Otherwise, an error message will be printed and you can retry again.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regex is Incorrect. Please try again\n"
     ]
    }
   ],
   "source": [
    "# Please do not modify the below line\n",
    "%run corrections.py\n",
    "\n",
    "# Please type in the regex below\n",
    "regex=\"\"\n",
    "\n",
    "#Please do not modify the line below\n",
    "print(checkEmailRegex(regex))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Named Entity Recognition (NER)\n",
    "\n",
    "### 2.1 What is Named Entity Recognition?\n",
    "\n",
    "In the previous chapter, we learnt about Information Extraction. One important technique under Information Extraction (IE) umbrella is Named Entity Recognition, popularly called as NER. \n",
    "\n",
    "In this technique, we try and identify the named entities mentioned in an unstructured text. The named entities could be anything ranging from a person's name to company's name like Apple Computers to city names like Mumbai. In simpler sense/words, you can imagine entities or named entities to be Nouns(Proper Noun to be more specific). Names of people like Peter, of companies like Microsoft, of places like Manhatten are all Named Entities.\n",
    "Commonly occurring Named Entites are:\n",
    "1. Person's Name\n",
    "2. Company / Organization Names\n",
    "3. City / State / Country Name (Location)\n",
    "\n",
    "### Applications\n",
    "\n",
    "There are bunch of areas in which NER Systems are used. Some of the use cases could be\n",
    "1. In support cases where in the fault in a product mentioned.\n",
    "2. In news, identify the main subjects in the news.\n",
    "3. To find a relation between various entities described in a document\n",
    "\n",
    "In short NER Systems are an integral part on NLP and using them can provide an insight into the data right away and its practical applications in day to day NLP tasks are immense.\n",
    "\n",
    "\n",
    "## How Does NER System work\n",
    "\n",
    "Most of the NER Systems today incorporate the following processes to identify the Named Entities in a sentence or any unstructured piece of text.\n",
    "1. Tokenization\n",
    "2. PoS Tagging\n",
    "3. Classification\n",
    "\n",
    "Lets get to know each of the processes in short. For the rest of the tutorial we will take the following piece of text for analysis. Its a summary of a popular Marvel Movie released recently.\n",
    "\n",
    "Captain Marvel is an extraterrestrial Kree warrior who finds herself caught in the middle of an intergalactic battle between her people and the Skrulls. Living on Earth in 1995, she keeps having recurring memories of another life as U.S. Air Force pilot Carol Danvers. With help from Nick Fury, Captain Marvel tries to uncover the secrets of her past while harnessing her special superpowers to end the war with the evil Skrulls.\n",
    "\n",
    "``` python\n",
    "text=\"Captain Marvel is an extraterrestrial Kree warrior who finds herself caught in the middle of an intergalactic battle between her people and the Skrulls. Living on Earth in 1995, she keeps having recurring memories of another life as U.S. Air Force pilot Carol Danvers. With help from Nick Fury, Captain Marvel tries to uncover the secrets of her past while harnessing her special superpowers to end the war with the evil Skrulls.\"\n",
    "\n",
    "```\n",
    "\n",
    "### 1. Tokenization\n",
    "\n",
    "Tokenization is the process in which the sentence or the text is broken down into individual words that make them. So the above text will be broken down into array of words like [\"Captain\",\"Marvel\",\"is\",\"an\",\"extraterrestrial\"....].\n",
    "Tokenization is the first step in preprocessing in which it eliminates all the white spaces, prepares a word array of the entire text and makes it ready for further processing.\n",
    "\n",
    "### 2.  Pos Tagging\n",
    "\n",
    "PoS or Parts of Speech tagging is the process in which each of the tokenized word, in step 1 is tagged as a Part of Speech like Noun, Verb, Adjective, Pronoun, Conjunction etc. A sample Pos Tagged word could be seen in the figure below. \n",
    "Also to decipher the short forms in the picture(or in further discussions) please consult the Pos Tag list at the link https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "<img src=\"../images/postagged.png\"/>\n",
    "\n",
    "### 3. Classification\n",
    "\n",
    "Post PosTagging, we take the output we get, and then run it through a classifier to identify the chunks as\n",
    "a Person, Organization, Location etc. \n",
    "Over the years various algorithms have been used for this right from rule/grammar based in the early days to Deep Learning based sophisticated algothims today. \n",
    "There are two popular algorithms that we use today are:\n",
    "1. Conditional Random Fields (Mostly in use today. Readers will learn more about this in the following section of the tutorial.)\n",
    "2. RNN based deep neural networks (Gaining lot of popularity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Named Entity Recognition in Practise Today\n",
    "\n",
    "Lot of work has been done in the area of Entity Recognition and there are some popular tools that are available in the market today. Some of them are:\n",
    "1. NLTK (Available in python)\n",
    "2. Spacy (Python)\n",
    "3. Stanford NER (Java, also available in python as an external package)\n",
    "\n",
    "We will take a look some code samples from NLTK and Spacy\n",
    "\n",
    "### NER using NLTK\n",
    "\n",
    "As mentioned earlier, the process of any NER system requires, tokenizing, POS tagging and finally identifying the named entities.\n",
    "Let's try to find out the Named Entites in our sample text\n",
    "\n",
    "``` python\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text_tokens = nltk.word_tokenize(text)\n",
    "#    The above statement breaks the text sample into tokens of independent words. For Example , the first line\n",
    "#    text would be broken down into something like\n",
    "#        \"Captain\",\"Marvel\",\"is\",\"an\",\"extraterrestrail\"........        \n",
    "    \n",
    "pos_tags = nltk.pos_tag(text_tokens)\n",
    "\n",
    "#    The above code will now tag the tokenized words into various Parts of Speech. An example of it can be seen in the \n",
    "#    following image\n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"../images/postagged.png\"/>\n",
    "\n",
    "```python\n",
    "\n",
    "\"\"\"    \n",
    "    We now use NER Classifier to Identify the named entities in the piece of unstructured text. Following is a sample\n",
    "    snippet to do so. Running the below snippet will produce the classified Named entity chunks (this is done \n",
    "    using an inbuilt classifier) which classifies the tagged text into Person, Organization etc. \n",
    "    The following figure shows this in more detail.\n",
    "\"\"\"\n",
    "ne_chunks = nltk.ne_chunk(pos_tags)\n",
    "for name in ne_chunks:\n",
    "    if hasattr(name, 'label'):\n",
    "        print(name.label(), ' - '.join(c[0] for c in name.leaves()))\n",
    "```\n",
    "<img src=\"../images/nechunk.png\"/>\n",
    "\n",
    "As you can see from the picture above, Marvel is recognized as a Person, Skrulls is recognized as an Organizations and Carol is recognized as a Person.\n",
    "\n",
    "### NER using Spacy\n",
    "\n",
    "Now lets quickly take a look at the NER output using another popular NER system which is gaining lot of popularity\n",
    "lately, Spacy. Following is a sample snippet\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "#The following line loads the English libraries needed for Spacy,viz, English tokenizer, tagger, parser, NER and word vectors\n",
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Parse the above text into Spacy's model. This will tokenize the words, pos tag them and identify the named\n",
    "#entities automatically.\n",
    "\n",
    "parsed = model(text)\n",
    "\n",
    "#Now, to find named entities\n",
    "for entity in parsed.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "    \n",
    "```\n",
    "\n",
    "The following figure contains the output of the program when run:\n",
    "\n",
    "<img src=\"../images/spacy.png\"/>\n",
    "\n",
    "__Lets see some of the insights that we can draw from our sample text :__\n",
    "\n",
    "```python\n",
    "# Gain more insight into the relationship of the words with each other. Following is a snippet to show the syntactic\n",
    "# dependencies\n",
    "from spacy import displacy\n",
    "displacy.serve(parsed, style='dep')\n",
    "\n",
    "#The output of the above is attached as an image for reference. The image is pretty wide, so it has been cropped \n",
    "#to fit here\n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"../images/dep_tree.png\"/>\n",
    "\n",
    "Now let's also try to plot a bar graph depicting the number of Entities which were found and the Type of those entities (like Person, Organization etc).\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "counter = {}\n",
    "for entity in parsed.ents:\n",
    "    if entity.label_ in counter.keys():\n",
    "        count = counter[entity.label_] + 1\n",
    "        counter[entity.label_] = count\n",
    "    else:\n",
    "        counter[entity.label_] = 1\n",
    "        \n",
    "plt.xlabel('Entity Types', fontsize=5)\n",
    "plt.ylabel('Num Occurences', fontsize=5)\n",
    "plt.bar(counter.keys(),counter.values())\n",
    "\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "<img src=\"../images/barr.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:\n",
    "### Find out Named Entities in a sample text\n",
    "This is an exercise in which the reader is expected to find out named entities from a sample text. We wil be using both NTLK and Spacy NER in the exercise.\n",
    "\n",
    "#### Instructions\n",
    "Please follow the instructions given below to complete the exercise.\n",
    "\n",
    "    1. Load the NER library\n",
    "    2. In case of NLTK:\n",
    "        a) Tokenize the text.\n",
    "        b) Find out the POS tags.\n",
    "        c) Use the POS tags to find out the Named Entities in the text.\n",
    "        \n",
    "    3. In case of Spacy:\n",
    "        a) Load the English model.\n",
    "        b) Pass the text to the loaded model to get the Named Entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE American\n",
      "ORGANIZATION Marvel - Comics\n",
      "ORGANIZATION Avengers\n",
      "PERSON Marvel - Studios\n",
      "PERSON Walt - Disney - Studios\n",
      "PERSON Marvel\n",
      "ORGANIZATION Avengers\n",
      "GPE Ultron\n",
      "ORGANIZATION Marvel - Cinematic - Universe\n",
      "ORGANIZATION MCU\n",
      "PERSON Anthony\n",
      "PERSON Joe - Russo\n",
      "PERSON Christopher - Markus\n",
      "PERSON Stephen - McFeely\n",
      "PERSON Robert - Downey - Jr.\n",
      "PERSON Chris - Evans\n",
      "PERSON Mark - Ruffalo\n",
      "PERSON Chris - Hemsworth\n",
      "PERSON Scarlett - Johansson\n",
      "PERSON Jeremy - Renner\n",
      "PERSON Don - Cheadle\n",
      "PERSON Paul - Rudd\n",
      "PERSON Brie - Larson\n",
      "PERSON Karen - Gillan\n",
      "PERSON Danai - Gurira\n",
      "PERSON Bradley - Cooper\n",
      "PERSON Josh - Brolin\n",
      "ORGANIZATION Avengers\n",
      "GPE Thanos\n",
      "GPE Infinity\n",
      "GPE Russo\n",
      "PERSON Markus\n",
      "PERSON Marvel\n",
      "ORGANIZATION Avengers\n",
      "ORGANIZATION Pinewood - Atlanta - Studios\n",
      "GPE Fayette - County\n",
      "GPE Georgia\n",
      "ORGANIZATION Infinity - War\n",
      "ORGANIZATION Metro\n",
      "PERSON Downtown - Atlanta\n",
      "GPE New - York\n",
      "American NORP\n",
      "the Marvel Comics ORG\n",
      "Marvel Studios ORG\n",
      "Walt Disney Studios Motion Pictures ORG\n",
      "2018 DATE\n",
      "2012 DATE\n",
      "Marvel's The Avengers WORK_OF_ART\n",
      "2015 DATE\n",
      "Ultron ORG\n",
      "22nd DATE\n",
      "the Marvel Cinematic Universe WORK_OF_ART\n",
      "MCU ORG\n",
      "Anthony PERSON\n",
      "Joe Russo PERSON\n",
      "Christopher Markus PERSON\n",
      "Stephen McFeely PERSON\n",
      "Robert Downey Jr. PERSON\n",
      "Chris Evans PERSON\n",
      "Mark Ruffalo PERSON\n",
      "Chris Hemsworth PERSON\n",
      "Scarlett Johansson PERSON\n",
      "Jeremy Renner PERSON\n",
      "Don Cheadle PERSON\n",
      "Paul Rudd PERSON\n",
      "Brie Larson PERSON\n",
      "Karen Gillan PERSON\n",
      "Danai Gurira PERSON\n",
      "Bradley Cooper PERSON\n",
      "Josh Brolin PERSON\n",
      "Thanos PERSON\n",
      "Infinity War EVENT\n",
      "October 2014 DATE\n",
      "Infinity War – Part 2 EVENT\n",
      "Russo NORP\n",
      "April 2015 DATE\n",
      "May DATE\n",
      "Markus ORG\n",
      "McFeely ORG\n",
      "July 2016 DATE\n",
      "Untitled Avengers ORG\n",
      "August 2017 DATE\n",
      "Pinewood Atlanta Studios ORG\n",
      "Fayette County GPE\n",
      "Georgia GPE\n",
      "Infinity War EVENT\n",
      "January 2018 DATE\n",
      "Metro FAC\n",
      "Atlanta GPE\n",
      "New York GPE\n",
      "December 2018 DATE\n"
     ]
    }
   ],
   "source": [
    "#Import NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "#Import NLTK End\n",
    "\n",
    "#Import Spacy\n",
    "import spacy\n",
    "model = spacy.load('en_core_web_sm')\n",
    "#Import Spacy End\n",
    "\n",
    "text = \"Avengers: Endgame is an upcoming American superhero film based on the Marvel Comics superhero team the Avengers, produced by Marvel Studios and set for distribution by Walt Disney Studios Motion Pictures. It is the direct sequel to 2018's Avengers: Infinity War, a sequel to 2012's Marvel's The Avengers and 2015's Avengers: Age of Ultron, and the 22nd film in the Marvel Cinematic Universe (MCU). The film is directed by Anthony and Joe Russo with a screenplay by Christopher Markus and Stephen McFeely and features an ensemble cast including Robert Downey Jr., Chris Evans, Mark Ruffalo, Chris Hemsworth, Scarlett Johansson, Jeremy Renner, Don Cheadle, Paul Rudd, Brie Larson, Karen Gillan, Danai Gurira, Bradley Cooper, and Josh Brolin. In the film, the surviving members of the Avengers and their allies work to reverse the damage caused by Thanos in Infinity War.The film was announced in October 2014 as Avengers: Infinity War – Part 2. The Russo brothers came on board to direct in April 2015, and by May, Markus and McFeely signed on to script the film. In July 2016, Marvel removed the title, referring to it simply as Untitled Avengers film. Filming began in August 2017 at Pinewood Atlanta Studios in Fayette County, Georgia, shooting back-to-back with Infinity War, and ended in January 2018. Additional filming took place in the Metro and Downtown Atlanta areas and New York. The official title was revealed in December 2018.\"\n",
    "\n",
    "#NLTK specific Code start\n",
    "\n",
    "text_tokens = nltk.word_tokenize(text)    \n",
    "pos_tags = nltk.pos_tag(text_tokens)\n",
    "ne_chunks = nltk.ne_chunk(pos_tags)\n",
    "for name in ne_chunks:\n",
    "    if hasattr(name, 'label'):\n",
    "        print(name.label(), ' - '.join(c[0] for c in name.leaves()))\n",
    "#NLTK specific Code end\n",
    "\n",
    "#Spacy specific Code start\n",
    "parsed = model(text)\n",
    "\n",
    "for entity in parsed.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "#Spacy specific code end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Condition Random Fields\n",
    "\n",
    "\n",
    "### 3.1 Introduction to CRF\n",
    "Named Entity Recognition has been one of very active area of study in Information Extraction. It is because of the challanges it poses and the necessary information that can be extracted out of a piece of unstructured text. Hence, lot of tools have been successfully applied for extracting them out. However many of these tools have difficulty modelling ovelapping, non-independant features, like factoring in the tags(POS) of the surrounding words, their capitalization patterns etc, which in essence give lot of insight about the structure of the sentence (in English language atleast). Let us try to understand this in some detail.\n",
    "\n",
    "__Tesla said, he is not going to Spain.__\n",
    "\n",
    "Let us try to analyze the above sentence from a non technical perspective. Following are the conclusions that we can draw after reading the sentence.\n",
    "\n",
    "    * The subject of the sentence is a proper noun (Tesla).\n",
    "    * The verb are: said, going\n",
    "    * The predicate of the sentence is said, he is not going to Spain.\n",
    "    * There is one more proper noun in the sentence (Spain)\n",
    "    \n",
    "If we look at the above sentence more closely, we can also make the following observations:-\n",
    "\n",
    "    * Only two words are Capitalized in the sentence, and they are both Proper Nouns (or Named Entities)\n",
    "    * If a verb succeeds a capitalized word, then the word is a Noun.\n",
    "    \n",
    "Now, if we notice, the last two observations we made, involved considering all the words in the sentence and their characteristics, like whether or not the next word is a VERB, which of the words have the first letter in Caps etc. We will call this information(and similar information) as the contextual information. These are kind of features that are dependent on the preceeding or the succeeding observation (word) in the sentence.\n",
    "\n",
    "__Condition Random Fields(CRF)__ are <a href=\"https://en.wikipedia.org/wiki/Discriminative_model\" target=\"_blank\">discriminitive</a> <a href=\"https://en.wikipedia.org/wiki/Graphical_model\" target=\"_blank\"> graphical models</a> that can model these overlapping (or correlation) between the features. It is a sequence modelling tool which factors in the correlation and predicts the conditional probablity. CRF defines the conditional probablity of a tag, given the word,i.e, given the word in a sentence / text, what is the probablity that it belongs to a certain class(tag) of named entities, like Person, Organization, Location etc. \n",
    "\n",
    "For simplicity, we will be studying a special case of CRF, linear chain CRF. Mathematically, it can be denoted as:\n",
    "\n",
    "$P(Z|X)=\\frac{1}{K}exp(\\sum_{n=1}^N\\sum_{i=1}^F\\lambda_if_i(z_{n-1},z_n,X,n))$\n",
    "\n",
    "where,\n",
    "\n",
    "Z = { $z_1,z_2...z_n$ } is a set of N labels(tags) like Person, Organization, Location etc. It is the classes which need to be predicted by the CRF model\n",
    "\n",
    "X = {$x_1,x_2...x_n$} is a set of N observations(words) in the sentence / text.\n",
    "\n",
    "$f_i$ is the feature function.\n",
    "\n",
    "$\\lambda_i$ is the learning parameters and weight of the feature function $f_i$.\n",
    "\n",
    "K is the partition function(or normalization function) which is used to normalize the value of the function in the range of (0,1) so as to make it a valid probablity value. Mathematically $K$ is the sum of the feature function over all the observations and it is defined as:\n",
    "\n",
    "$K=\\sum_Zexp(\\sum_{n=1}^N\\sum_{i=1}^F\\lambda_if_i(z_{n-1},z_n,X,n))$\n",
    "\n",
    "\n",
    "## Feature Function\n",
    "\n",
    "Feature function are the central piece to CRF because it is this function that provides the contextual information to the CRF model. In linear-chain CRF, the feature function is denoted by $f_i(z_{n-1},z_n,X,n)$. It means that the function embodies in it the following information\n",
    "\n",
    "    * The previous and the next state information or in our example we can call this as tags(z).\n",
    "    * All the input words in the sentence / text.\n",
    "    * The position of the word in the sentence / text.\n",
    "    \n",
    "For example, we can define a simple feature function to produce binary values,i.e, 1 if the current word is Tesla and if the current state is Person.\n",
    "\n",
    "$f_1(z_{n-1},z_n,X,n)=\\begin{cases}1&\\text{if z_n = Person and x_n = Tesla}\\\\0&\\text{Otherwise}\\end{cases}$\n",
    "\n",
    "The way the above feature will get used depends directly on the weight associated with the feature $\\lambda_1$. Whenever $\\lambda_1>0$ , it increases the probablity that whenever the word 'Tesla' is encountered, the model would prefer the tag 'Person' for it. Similarly, if the value of $\\lambda<0$, the model would avoid associating the tag 'Person' to the word 'Tesla'. And finally, if $\\lambda=0$, it would not alter the behavior of the model in any way,i.e, it will have no effect.\n",
    "\n",
    "Let us define another feature function which say, tries to establish the relationship between the tag 'Person' and when followed by the word 'said', for example __Tesla said,__ :-\n",
    "\n",
    "$f_2(z_{n-1},z_n,X,n)=\\begin{cases}1&\\text{if z_n = Person and x_n+1 = said}\\\\0&\\text{Otherwise}\\end{cases}$\n",
    "\n",
    "In the above feature function, $\\lambda_2>0$ whenver 'said' succeeds a words with tag 'Person'.\n",
    "\n",
    "Now if we compare the above feature functions, we find that it can both be applied to our feature functions $f_1$ and $f_2$ for the example __Tesla said,..__. This is an example of overlapping features wherein both the feature functions get activated on a particular input set. It boosts the probablity of the word 'Tesla' being classified as 'Person'.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "We discussed feature functions is detail. Now let us take a look at how do we arrive at selection of these features for a given NER task. Mostly the selection of what features to use involves lot of techniques used in feature engineering. However, one can always start with some features and build more along the way\n",
    "\n",
    "* One of the features could be a simple combination of words and tags. For example (x= Tesla, z= Person), (x=SpaceX, z= Organization)\n",
    "\n",
    "* Another feature set could be the position of the capitalized words in the sentence. \n",
    "\n",
    "* Another feature could be the neighbouring words and their POStags.\n",
    "\n",
    "So, the idea is to select the candidate features that would help increase the effectiveness of the CRF model.\n",
    "\n",
    "\n",
    "## Applications of CRF\n",
    "\n",
    "Typically CRFs find its use in processing correlated sequential data, like in identifying parts of speech in a sentence. Parts of speech of a sentence rely of positioning of the word in the sentence, like which word is before or after a certain word in a sentence, and by using features that take advantage of this, we can use CRF to learn how to distinguish which word belongs to which part of speech.\n",
    "\n",
    "Another very popular application of CRFs is in Named Entity Recognition, where it is used to extract proper nouns in a sentence and then classify it into whether the proper noun is a Person or a Place or a Company etc.\n",
    "\n",
    "It also finds use in Computer Vision for image segmentation and analysis. It has also been used to identify objects and its attributes in images.(as shown below)\n",
    "\n",
    "<img src='../images/crf_image.png'/>\n",
    "\n",
    "Image source: https://www.oreilly.com/library/view/deep-learning-for/9781788295628/a64a86ee-4873-4833-9196-81c70e9e3389.xhtml\n",
    "        \n",
    "### 3.2 CRF using Python\n",
    "\n",
    "In the section above, we gained a deeper insight into Condition Random Fields and how it works and its applications. In this section we will see how we can implement our own CRF using python.\n",
    "\n",
    "To implement CRF using Python, we will be using the following libraries(available in Python):-\n",
    "\n",
    "    * Pandas (pandas) - To read the dataset and use the its 'dataframe' datastructures for processing\n",
    "    * Sklearn (sklearn) - We will using sklearn's APIs for splitting the given dataset into train and test datasets so that we can train our model on one dataset and test it out on the other.\n",
    "    * Sklearn CRF suite (sklearn_crfsuite) - We will also be using CRF suite's CRF implementation API. It is always helpful in learning and in using in practical applications to use a well written and tested implementation of any machine learning algorithm.\n",
    "    \n",
    "We will now acquaint ourselves with the necessary steps that we need to do to use CRF in python.\n",
    "\n",
    "\n",
    "1. Import Pandas, Sklearn and CRF Suite from SKlearn libraries in python for later use\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import sklearn_crfsuite\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "2. Once we have the dataset, we use the loaded libraries to read the dataset from csv file and put it in\n",
    "a dataframe (A pandas datastructure)\n",
    "\n",
    "```python\n",
    "data_from_csv = pd.read_csv(\"../data/ner.csv\",encoding = \"ISO-8859-1\",error_bad_lines=False)\n",
    "```\n",
    "The above snippet will load the contents of the csv file in a variable of type 'dataframe' and assign it to the\n",
    "variable data_from_csv\n",
    "\n",
    "3. Once we have the data, our next step is to prepare the data in a way that we can feed it to the crfsuite \n",
    "API of sklearn library. The inpuit to the api is a set of feature object which consists of the following features:\n",
    "  1. Whether or not the word is in lower case\n",
    "  2. The adjacent words to the word.\n",
    "  3. Where or not the word is in upper case.\n",
    "  4. Whether or not the word is a title or is a heading in the text.\n",
    "  5. If the word consist of digits only.\n",
    "  6. The POS tags of the word.\n",
    "  7. The POS tags of the adjacent words.\n",
    "\n",
    "4. Once we have processed the data, we need to train our CRF algorithm with the data. The CRF algorithm of the \n",
    "sklearn API can be initialized in the following way:-\n",
    "\n",
    "```python\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "```\n",
    "\n",
    "5. Once we have the model initialized, we will then call the fit() function of the CRF model to train the values. The function take the folowing parameters:\n",
    "\n",
    "    1. X -> Features to be trained.\n",
    "    2. Y -> The class labels against which the training needs to be done.\n",
    "\n",
    "```python\n",
    "crf.fit(X,Y)\n",
    "```\n",
    "\n",
    "6. The above function trains the CRF algorithm in the dataset provided. Once trained, we will use the model to predict the values from the test dataset. The following snippet would be used for this.\n",
    "\n",
    "```python\n",
    "Y = crf.predict(X_test)\n",
    "```\n",
    "\n",
    "We pass the untrained data to the model and we get the output as the predicted class label (Y). Voila !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Named Entity Recognition using Deep Learning\n",
    "\n",
    "### 4.1 Model Architecture\n",
    "\n",
    "In the previous chapter, we saw how we could extract named entities from raw text using CRF. In this chapter we will see how we can do the same using Deep Learning. To do this, we will be using a very popular Deep learning framework in python, known as Keras. We will be using a special type of RNN,i.e, Bi-LSTM(Bi-directional Long short term momory) for this tutorial. More on Bi-LSTM in the following topic.\n",
    "\n",
    "The dataset used in this tutorial will be the same as that in the previous tutorial so that the readers could corelate things easily. We will be modifying the input feature code a bit to make it compatible with the Keras engine requirements.\n",
    "\n",
    "## Bi-LSTM Cell\n",
    "\n",
    "Bi-LSTM cells are a special form of LSTM(Long Short Term Memory) cells. Psst, if you are not familier with LSTMs or RNNs in general, I recommend you to read the following blogs:\n",
    "\n",
    "For RNN -> http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "For LSTM -> https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "\n",
    "Bi-Directional are in essence two sets of LSTM cells working together. One set of them pass/store the contextual information from backwards to forward, whereas the other set passes information from forward to backward,i.e, from future to past. The following figure would help in gaining a better understanding of Bi-LSTM cells.\n",
    "\n",
    "<img src='../images/lstm_bilstm.png'/>\n",
    "\n",
    "## NER using Keras\n",
    "\n",
    "To find out Named Entities, we will be using a high level neural network API called Keras. Using Keras, gives us 2 major benefits:\n",
    "\n",
    "    * We do not have to delve into low level implementation of Bi-LSTM and rather concentrate on application specific problem,\n",
    "    \n",
    "    * Keras works on top of many of the popular backends like Tensorflow, CNTK, Theanos etc and exposes the high level APIs for ease of use. \n",
    "\n",
    "The overall architecture of how the various components work together can be seen in the flow diagram below.\n",
    "\n",
    "<img src='../images/deeplearning_flow.png'/>\n",
    "\n",
    "\n",
    "Let's now see what are the steps needed for the identifying Named Entities using Keras.\n",
    "\n",
    "1. The first and formost step is to import all the necessary libraries that we are going to need for our exercice. We are going to import the following libraries:-\n",
    "    \n",
    "    *Numpy (For numerical calculations)\n",
    "    \n",
    "    *Pandas (Reading and processing our dataset)\n",
    "    \n",
    "    *Sklearn (Many helper libraries to process the dataset, split the train and test data etc)\n",
    "    \n",
    "    *Keras (for Bi-LSTM implementation )\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "```\n",
    "\n",
    "2. Post all imports, we use the loaded libraries to read the dataset from csv file and put it in\n",
    "a dataframe (A pandas datastructure)\n",
    "\n",
    "```python\n",
    "data_from_csv = pd.read_csv(\"../data/ner.csv\",encoding = \"ISO-8859-1\",error_bad_lines=False)\n",
    "```\n",
    "\n",
    "The above snippet will load the contents of the csv file in a variable of type 'dataframe' and assign it to the\n",
    "variable data_from_csv\n",
    "\n",
    "3. Once the dataset is loaded, we process the dataset as per the input requirement of the Keras API. The processing of the data is going to be similar to what we did in CRF section where in we extract all the words along with their POS tags.\n",
    "\n",
    "Once we have the dataset, we are going to pad the dataset to the fixed length input vector. This is done primarily to make it consistant with the requirements of the neural network input layer that the all the input vectors have the same dimensionality. The following snippet pads the input vector \n",
    "\n",
    "```python\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "```\n",
    "\n",
    "4. Once we have the data ready, we split it into training data and test data. As the name suggests, the test data is to test the efficiency and accuracy of our model after training.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "```\n",
    "\n",
    "5. We are now ready to feed the data to Keras' Bi-LSTM API. Let's try to understand the process in detail.\n",
    "\n",
    "    * The first step is to instantiate a Keras Tensor. A Keras tensor is a tensor object from the underlying backend( in our example, Tensorflow. Could be CNTK / Theano as well). We initialize this with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model. The following snippet initialized a Keras model\n",
    "```python\n",
    "input = Input(shape=<\"A shape tuple\">)\n",
    "```\n",
    "\n",
    "As you can see the Input() takes a minimum of the dimensionality of the input vectors as parameter.\n",
    "\n",
    "\n",
    "    * We will now get the word embeddings for the input data. Word embeddings, on a very high level, are a set of feature vector that defines every word in the text. So every input word is converted into a set of feature vectors denoting that word. The Embedding layer in Keras is defined as the first hidden layer of the network. It must specify 3 arguments:-\n",
    "    \n",
    "    > Input Dimension (input_dim) : Size of the vocabulary of the text data.\n",
    "    > Output Dimenstion (output_dim) : The size of the vector space in which all the words will be embedded.\n",
    "    > Input Length(input_length) : The length of the input sequences.\n",
    "\n",
    "```python\n",
    "embedding = Embedding(input_dim=n_words, output_dim=50, input_length=max_len)\n",
    "```\n",
    "Once we have the word embeddings for each of the words in the input text, we feed this to the Bidirectional API.\n",
    "\n",
    "    * We use the Bidirectional API to initlize the LSTM cells. The Bidirectinal API is just a wrapped API sitting on top of Kera's default RNN API implementation. In here we primarily specify the number of LSTM cells that we need to initialize and additional parameters. Following are the parameters that it takes:\n",
    "    > layer : A Recurrent instance . The Recurrent instance that we pass in our case is that of an LSTM cell. The LSTM cell takes in host of parameters, but the primary one is number of LSTM cells that need to be deployed.\n",
    "\n",
    "\n",
    "```python\n",
    "Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))\n",
    "```\n",
    "\n",
    "     * Once we have added the implementation of the API, we must prepare the output layer. To prepare this, we pass a Dense( a densely connected NN layer) to a TimeDistributed layer. Now in Keras, the second dimension is related to time dimension. This means that if your data is n-dimensional, you could apply a TimeDistributed, which is applicable to (n-1) dimensions. The TimeDistributed wrapper allows to apply a layer to every temporal slice in the input.\n",
    "\n",
    "```python\n",
    "out = TimeDistributed(Dense(len(tags), activation=\"softmax\"))(model)  # softmax output layer\n",
    "```\n",
    "\n",
    "    * Now, we need to compile our model and specify the loss functions etc.\n",
    "```python\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "```\n",
    "\n",
    "    * Once the model is compiled, we train the model using the input data as shown in the following:\n",
    "    \n",
    "```python\n",
    "trained = model.fit(X_train, Y batch_size=<batchsize>, epochs=5, validation_split=0.1, verbose=1)\n",
    "```\n",
    "\n",
    "We then use the trained model to predict the named entities from text. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
